<!DOCTYPE html>
<html lang="en">

<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8">
	<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
	
	<!-- title -->
	
	<title>
	
		SeaTunnel开源数据同步平台 | 
	 
	佳境的技术专区
	</title>
	
	<!-- keywords,description -->
	
		<meta name="keywords" content="SeaTunnel," />
	 
		<meta name="description" content="SeaTunnel数据抽取工具" />
	

	<!-- favicon -->
	
	<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/Shmilyqjj/Shmilytech.github.io@hexo/themes/hexo-theme-tree/source/favicon.ico">
	
  

	
<link rel="stylesheet" href="/css/main.css">

	
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css">

	
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/styles/darcula.min.css">


	
<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/highlight.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/npm/jquery-pjax@2.0.1/jquery.pjax.min.js"></script>

	
<script src="/js/main.js"></script>

	
		
<script src="https://cdn.jsdelivr.net/npm/leancloud-storage/dist/av-min.js"></script>

		
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>

	
	
		<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	
<meta name="generator" content="Hexo 4.2.1"></head>

<body>
	<header id="header">
    <a id="title" href="/" class="logo">佳境的技术专区</a>
	<ul id="menu">
		<li class="menu-item">
			<a href="https://shmily-qjj.top/" class="menu-item-link" target="_self">
				<input type="image" src="https://blog-images-1257889704.cos.ap-chengdu.myqcloud.com/Resources/img/logo/gotoMain.png" width="90" height="35" alt="回到主站"/>
			</a>
		</li>
		<li class="menu-item">
			<a href="https://shmily-qjj.top/about/" target="_blank" rel="noopener" class="menu-item-link">
				<input type="image" src="https://blog-images-1257889704.cos.ap-chengdu.myqcloud.com/Resources/img/logo/aboutMe.png" width="90" height="35" alt="关于我"/>
			</a>
		</li>

		<li class="menu-item">
			<a href="https://github.com/Shmilyqjj" class="menu-item-link" target="_blank">
<!--				<i class="fa fa-github fa-2x"></i>-->
				<input type="image" src="https://blog-images-1257889704.cos.ap-chengdu.myqcloud.com/Resources/img/logo/myGithub.png" width="90" height="35" alt="关于我"/>
			</a>
		</li>
	</ul>
</header>

	
<div id="sidebar">
	<button id="sidebar-toggle" class="toggle" ><i class="fa fa-arrow-right " aria-hidden="true"></i></button>
	
	<div id="site-toc">
		<input id="search-input" class="search-input" type="text" placeholder="search...">
		<div id="tree">
			

			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										01数据结构与算法
									</a>
									
							<ul>
								<li class="file">
									<a href="/6a894937/">
										基础算法学习
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										02计算机网络
									</a>
									
							<ul>
								<li class="file">
									<a href="/39a9ed67/">
										Linux Bind服务配置DNS解析
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										03操作体统
									</a>
									
							<ul>
								<li class="file">
									<a href="/3f34ebe3/">
										基于Manjaro KDE版打造美观舒适开发环境
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										04编程语言
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										Java
									</a>
									
							<ul>
								<li class="file">
									<a href="/508b5c7/">
										系统学习JVM
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/6f97dc89/">
										线程进程与锁
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										Python
									</a>
									
							<ul>
								<li class="file">
									<a href="/2ed52290/">
										高效运行Python方案
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										05数据库
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										MySQL
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										原理深入
									</a>
									
							<ul>
								<li class="file">
									<a href="/7c15e85/">
										MySQL索引原理深入
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/1f7eb1b3/">
										数据库事务ACID理解
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										操作
									</a>
									
							<ul>
								<li class="file">
									<a href="/3c26421b/">
										Mysql Event Scheduler
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/96009187/">
										浅谈group by与distinct去重
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										06分布式
									</a>
									
							<ul>
								<li class="file">
									<a href="/4a17b156/">
										hello-world
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										07容器
									</a>
									
							<ul>
								<li class="file">
									<a href="/4a17b156/">
										hello-world
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										08大数据
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										交互查询
									</a>
									
							<ul>
								<li class="file">
									<a href="/5f26355/">
										Apache Kudu总结
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/1ae37d82/">
										Impala-基于内存的高效SQL交互查询引擎
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/4c197c46/">
										Presto-基于内存的高效SQL交互查询引擎
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										其他
									</a>
									
							<ul>
								<li class="file">
									<a href="/4b21953d/">
										分享我的技术调研流程
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/39595/">
										记一次参加QCon全球软件开发大会
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="file">
									<a href="/BigdataExceptionsSummary/">
										大数据平台常见异常处理汇总
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										平台运维
									</a>
									
							<ul>
								<li class="file">
									<a href="/38328/">
										CentOS7安装CDH6全程记录
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										数据仓库
									</a>
									
							<ul>
								<li class="file active">
									<a href="/84534d72/">
										SeaTunnel开源数据同步平台
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/26078/">
										Sqoop学习笔记
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										数据可视化
									</a>
									
							<ul>
								<li class="file">
									<a href="/174820fd/">
										Apache Zeppelin初探
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										数据安全
									</a>
									
							<ul>
								<li class="file">
									<a href="/f5da73a2/">
										大数据脱敏方案调研
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/4cf161e5/">
										实现基于Spark的数据脱敏
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										数据湖
									</a>
									
							<ul>
								<li class="file">
									<a href="/44511/">
										Alluxio-基于内存的虚拟分布式存储系统
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/38dd005e/">
										Iceberg数据湖探索与实践
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										离线计算
									</a>
									
							<ul>
								<li class="file">
									<a href="/7fbbfd34/">
										Hive3.x新特性
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/ee1c2df4/">
										Kyuubi原理与替代SparkThriftServer实践
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/pyspark_pandas/">
										使用PySpark优化Pandas
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="file">
									<a href="/welcome/">
										欢迎
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
		</div>
	</div>
</div>

	<!-- 引入正文 -->
	<div id="content">
		<h1 id="article-title">

	SeaTunnel开源数据同步平台
</h1>
<div class="article-meta">
	
	<span>佳境Shmily</span>
	<span>2021-12-15 16:30:00</span>
    
		<div id="article-categories">
            
                
                    <span>
                        <i class="fa fa-folder" aria-hidden="true"></i>
                        <a href="/categories/技术/">技术</a>
						
                    </span>
                
            
		</div>
    
</div>

<div id="article-content">
	<h1 id="SeaTunnel开源数据同步平台"><a href="#SeaTunnel开源数据同步平台" class="headerlink" title="SeaTunnel开源数据同步平台"></a>SeaTunnel开源数据同步平台</h1><h2 id="SeaTunnel简介"><a href="#SeaTunnel简介" class="headerlink" title="SeaTunnel简介"></a>SeaTunnel简介</h2><p>SeaTunnel is a very easy-to-use ultra-high-performance distributed data integration platform that supports real-time synchronization of massive data.<br>SeaTunnel是一个简单易用且高效的开源数据集成平台（前身是WaterDrop），支持离线和实时数据同步。支持多种Source、Output、Filter组件以及自行开发输入输出插件和过滤器插件。SeaTunnel配置简单，基于已有的Spark、Flink环境几分钟就可以部署完成。因其有各种灵活的插件支持，只需要花几分钟编写一个配置文件即可完成一个数据同步任务的开发。</p>
<p>SeaTunnel架构<br><img src="https://blog-images-1257889704.cos.ap-chengdu.myqcloud.com/BlogImages/SeaTunnel/Seatunnel-01.png" alt="alt ">  </p>
<p>SeaTunnel特性：</p>
<ol>
<li>简单易用，配置灵活，低代码</li>
<li>支持实时数据流和离线数据同步</li>
<li>高性能分布式、海量数据处理能力</li>
<li>模块化、插件化，易于扩展</li>
<li>支持通过SQL做ETL操作</li>
</ol>
<p>SeaTunnel支持的组件：<br><strong>Input plugin：</strong> Fake, File, HDFS, Kafka, S3, Hive, Kudu, MongoDB, JDBC, Alluxio, Socket, self-developed Input plugin<br><strong>Filter plugin：</strong> Add, Checksum, Convert, Date, Drop, Grok, Json, Kv, Lowercase, Remove, Rename, Repartition, Replace, Sample, Split, Sql, Table, Truncate, Uppercase, Uuid, Self-developed Filter plugin<br><strong>Output plugin:</strong> Elasticsearch, File, Hdfs, Jdbc, Kafka, Mysql, S3, Stdout, self-developed Output plugin<br>支持的所有组件可以参考<a href="https://interestinglab.github.io/seatunnel-docs/#/zh-cn/v1/configuration/base" target="_blank" rel="noopener">SeaTunnel通用配置</a></p>
<h2 id="使用SeaTunnel"><a href="#使用SeaTunnel" class="headerlink" title="使用SeaTunnel"></a>使用SeaTunnel</h2><h3 id="安装部署SeaTunnel"><a href="#安装部署SeaTunnel" class="headerlink" title="安装部署SeaTunnel"></a>安装部署SeaTunnel</h3><p>使用SeaTunnel将Kudu数据导入ClickHouse<br>下载SeaTunnel:<a href="https://github.com/InterestingLab/SeaTunnel/releases" target="_blank" rel="noopener">SeaTunnel二进制包</a></p>
<pre><code class="shell">unzip seatunnel-1.5.5.zip
cd seatunnel-1.5.5
# 修改seatunnel环境配置
vim config/seatunnel-env.sh
SPARK_HOME=/hadoop/bigdata/spark/spark-2.3.2-bin-hadoop2.6</code></pre>
<h3 id="SeaTunnel将Kudu表导入ClickHouse"><a href="#SeaTunnel将Kudu表导入ClickHouse" class="headerlink" title="SeaTunnel将Kudu表导入ClickHouse"></a>SeaTunnel将Kudu表导入ClickHouse</h3><p>准备kudu表<br>Kudu表kudu_db.kudu_table（在KuduWebUI中表名为impala::kudu_db.kudu_table）<br><img src="https://blog-images-1257889704.cos.ap-chengdu.myqcloud.com/BlogImages/SeaTunnel/Seatunnel-02.png" alt="alt ">  </p>
<p>预先创建目标ClickHouse表</p>
<pre><code class="clickhouse-sql">CREATE TABLE test.ch_table
(
    `cust_no` String,
    `tag_code` String,
    `update_datetime` DateTime
)
ENGINE = MergeTree
ORDER BY cust_no;</code></pre>
<p>参考<a href="https://interestinglab.github.io/seatunnel-docs/#/zh-cn/v1/configuration/base" target="_blank" rel="noopener"><strong>seatunnel-docs-configuration</strong></a> 配置数据抽取任务<br>vim config/kudu2ch.batch.conf内容如下</p>
<pre><code class="config">spark {
  spark.app.name = &quot;kudu2ch&quot;
  # executor的数量
  spark.executor.instances = 2
  # 每个excutor核数 (并行度,数据量大可以适当增大到ClickHouse服务器核数一半以下,尽量不要影响ClickHouse)
  spark.executor.cores = 1
  # 每个excutor内存
  spark.executor.memory = &quot;1g&quot;
}
input {
 kudu{
   kudu_master=&quot;kudu_master1_ip:7051,kudu_master2_ip:7051,kudu_master3_ip:7051&quot;
   kudu_table=&quot;impala::kudu_db.kudu_table&quot;
  # 对应输出中需要指定source_table_name=&quot;kudu_table_source&quot;
   result_table_name=&quot;kudu_table_source&quot;
 }
}
filter {
}
output {
 clickhouse {
    # 指定从哪个源抽取数据
    source_table_name=&quot;kudu_table_source&quot;
    host = &quot;ch_jdbc_ip:8123&quot;
    clickhouse.socket_timeout = 50000
    database = &quot;test&quot;
    table = &quot;ch_table&quot;
    fields = [&quot;cust_no&quot;,&quot;tag_code&quot;,&quot;update_datetime&quot;]
    username = &quot;default&quot;
    password = &quot;123456&quot;
    # 每批次写入ClickHouse数据条数
    bulk_size = 20000
 }
}</code></pre>
<p>执行抽取任务：</p>
<pre><code class="shell">/opt/seatunnel-1.5.5/bin/start-seatunnel.sh --master local[3] --deploy-mode client --config /opt/seatunnel-1.5.5/config/kudu2ch.batch.conf</code></pre>
<p>排错1：</p>
<pre><code class="log">Caused by: ru.yandex.clickhouse.except.ClickHouseException: ClickHouse exception, code: 210, host: ch_jdbc_ip, port: 8123; Connect to ch_jdbc_ip:8123 [/ch_jdbc_ip] failed: Connection refused (Connection refused)</code></pre>
<p>原因：CH Server端未开启远程访问权限<br>解决：开启CH Server支持远程访问的权限</p>
<p>排错2：</p>
<pre><code class="log">2021-12-22 15:23:47 ERROR TaskSetManager:70 - Task 2 in stage 0.0 failed 1 times; aborting job
Exception in thread &quot;main&quot; java.lang.Exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): java.lang.ClassCastException: java.sql.Timestamp cannot be cast to java.lang.String
        at io.github.interestinglab.waterdrop.output.batch.Clickhouse.renderBaseTypeStatement(Clickhouse.scala:351)
        at io.github.interestinglab.waterdrop.output.batch.Clickhouse.io$github$interestinglab$waterdrop$output$batch$Clickhouse$$renderStatementEntry(Clickhouse.scala:373)
        at io.github.interestinglab.waterdrop.output.batch.Clickhouse$$anonfun$io$github$interestinglab$waterdrop$output$batch$Clickhouse$$renderStatement$1.apply$mcVI$sp(Clickhouse.scala:403)
        at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
        at io.github.interestinglab.waterdrop.output.batch.Clickhouse.io$github$interestinglab$waterdrop$output$batch$Clickhouse$$renderStatement(Clickhouse.scala:391)
        at io.github.interestinglab.waterdrop.output.batch.Clickhouse$$anonfun$process$2.apply(Clickhouse.scala:187)
        at io.github.interestinglab.waterdrop.output.batch.Clickhouse$$anonfun$process$2.apply(Clickhouse.scala:162)
        at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:935)
        at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:935)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:109)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)</code></pre>
<p>原因：如果Kudu中表字段格式为Timestamp，需要在写入ClickHouse前先将Timestamp类型数据转换为字符串格式否则会写入错误。<br>相关Git Issue: <a href="https://github.com/InterestingLab/seatunnel/issues/848" target="_blank" rel="noopener">SeaTunnel-848</a><br>相关文档：<a href="https://interestinglab.github.io/seatunnel-docs/#/zh-cn/v1/configuration/output-plugins/Clickhouse?id=clickhouse%e7%b1%bb%e5%9e%8b%e5%af%b9%e7%85%a7%e8%a1%a8" target="_blank" rel="noopener">ClickHouse类型对照表</a><br>解决：写入ClickHouse之前需要通过SeaTunnel中的 <a href="https://interestinglab.github.io/seatunnel-docs/#/zh-cn/v1/configuration/filter-plugin" target="_blank" rel="noopener"><strong>Filter插件</strong></a> 中的 <a href="https://interestinglab.github.io/seatunnel-docs/#/zh-cn/v1/configuration/filter-plugins/Sql" target="_blank" rel="noopener"><strong>SQL</strong></a> 或者 <a href="https://interestinglab.github.io/seatunnel-docs/#/zh-cn/v1/configuration/filter-plugins/Convert" target="_blank" rel="noopener"><strong>Convert</strong></a> 插件将各字段转换为对应格式，否则会产生报错<br>注意：若配置中有filter插件且需要filter生效，则不要在output指定source_table_name这个选项，若指定了source_table_name的值等于input中result_table_name的值，则会绕过filter(filter不生效)<br>修改配置<br>vim config/kudu2ch.batch.conf内容如下</p>
<pre><code class="config">spark {
  spark.app.name = &quot;kudu2ch&quot;
  # executor的数量
  spark.executor.instances = 2
  # 每个excutor核数 (并行度,数据量大可以适当增大到ClickHouse服务器核数一半以下,尽量不要影响ClickHouse)
  spark.executor.cores = 1
  # 每个excutor内存
  spark.executor.memory = &quot;1g&quot;
}
input {
 kudu{
   kudu_master=&quot;kudu_master1_ip:7051,kudu_master2_ip:7051,kudu_master3_ip:7051&quot;
   kudu_table=&quot;impala::kudu_db.kudu_table&quot;
  # 对应输出中需要指定source_table_name=&quot;kudu_table_source&quot;
   result_table_name=&quot;kudu_table_source&quot;
 }
}
filter {
  sql {
       sql = &quot;select cust_no,tag_code,date_format(update_datetime, &#39;yyyy-MM-dd&#39;) as update_datetime from kudu_table_source&quot;
  }
}
output {
 clickhouse {
    # 指定从哪个源抽取数据
    # source_table_name=&quot;kudu_table_source&quot;
    host = &quot;ch_jdbc_ip:8123&quot;
    clickhouse.socket_timeout = 50000
    database = &quot;test&quot;
    table = &quot;ch_table&quot;
    fields = [&quot;cust_no&quot;,&quot;tag_code&quot;,&quot;update_datetime&quot;]
    username = &quot;default&quot;
    password = &quot;123456&quot;
    # 每批次写入ClickHouse数据条数
    bulk_size = 20000
 }
}</code></pre>
<p>若使用Convert模块，Filter中内容</p>
<pre><code class="text">filter {
  date{
      source_field = &quot;update_datetime&quot;
      target_field = &quot;update_datetime&quot;
      source_time_format = &quot;UNIX&quot;
      target_time_format = &quot;yyyy-MM-dd HH:mm:ss&quot;
  }
}</code></pre>
<p>执行抽取任务：</p>
<pre><code class="shell">/opt/seatunnel-1.5.5/bin/start-seatunnel.sh --master local[3] --deploy-mode client --config /opt/seatunnel-1.5.5/config/kudu2ch.batch.conf</code></pre>
<p>数据验证:<br>Kudu:<br>+———-+<br>| count(1) |<br>+———-+<br>| 714218   |<br>+———-+<br>Fetched 1 row(s) in 2.39s</p>
<p>ClickHouse:<br>Query id: 8d6bc13d-c49d-408a-8e07-3d2691e3ebbb<br>┌─count()─┐<br>│  714218 │<br>└─────────┘<br>1 rows in set. Elapsed: 0.003 sec. </p>
<p>但DateTime类型相差8小时，因为ClickHouse的DateTime时区问题，故可以在sql中对update_datetime字段值减去8*3600秒</p>
<pre><code class="config">filter {
  sql {
       sql = &quot;select cust_no, tag_code, date_format(cast(cast(update_datetime as int) - 8*3600 as timestamp), &#39;yyyy-MM-dd HH:mm:ss&#39;) as update_datetime from kudu_table_source&quot;
  }
}</code></pre>
<p>一开始想设置ClickHouse中DateTime时区为DateTime(‘Asia/Hong_Kong’)，但SeaTunnel不支持这格式，只能用默认的DateTime格式<br>注意：SeaTunnel抽取Kudu的SparkTask数等于Kudu表的Tablet数，建议给定Spark程序并行度为Tablet数的三分之一或二分之一。</p>
<h3 id="SeaTunnel将Impala表导入ClickHouse"><a href="#SeaTunnel将Impala表导入ClickHouse" class="headerlink" title="SeaTunnel将Impala表导入ClickHouse"></a>SeaTunnel将Impala表导入ClickHouse</h3><p>SeaTunnel支持Input类型没有Impala但有JDBC，支持任何JDBC数据源，Impala也属于JDBC数据源。<br>通过SeaTunnel可以将Impala管理的Kudu表、Hive表数据导出到其他存储引擎。</p>
<p>准备Impala Hive表<br>Impala表 default.qjj_test<br>+——+——–+———+<br>| name | type   | comment |<br>+——+——–+———+<br>| id   | int    |         |<br>| name | string |         |<br>+——+——–+———+</p>
<p>创建对应目标ClickHouse表</p>
<pre><code class="sql">CREATE TABLE default.qjj_test
(
    `id` int,
    `name` String
)
ENGINE = MergeTree
ORDER BY id;</code></pre>
<p>参考<a href="https://interestinglab.github.io/seatunnel-docs/#/zh-cn/v1/configuration/input-plugins/Jdbc" target="_blank" rel="noopener"><strong>SeaTunnel-docs-JDBC</strong></a>编写任务配置文件<br>配置文件/opt/seatunnel-1.5.5/config/impala2ch.batch.conf如下:</p>
<pre><code class="config">spark {
  spark.app.name = &quot;impala-jdbc-2-clickhouse-jdbc&quot;
  spark.executor.instances = 2
  spark.executor.cores = 1
  # 每个excutor内存
  spark.executor.memory = &quot;2g&quot;
}
input {
 jdbc {
     driver = &quot;com.cloudera.impala.jdbc41.Driver&quot;
     url = &quot;jdbc:impala://impalad_ip:21050/default&quot;
     table = &quot;(select * from qjj_test) as source_table&quot;
     # 或者直接写表名也可以table = &quot;qjj_test&quot;
     result_table_name = &quot;impala_table_source&quot;
     user = &quot;&quot;
     password = &quot;&quot;
 }
}
filter {
}
output {
 clickhouse {
    source_table_name=&quot;impala_table_source&quot;
    host = &quot;ch_jdbc_ip:8123&quot;
    clickhouse.socket_timeout = 50000
    database = &quot;default&quot;
    table = &quot;qjj_test&quot;
    username = &quot;default&quot;
    password = &quot;123456&quot;
    # 每批次写入ClickHouse数据条数
    bulk_size = 20000
 }
}</code></pre>
<p>将jdbc-jar放入seatunnel目录的plugins/my_plugins/lib目录<br>Impala-jdbc下载地址：<a href="https://github.com/Shmilyqjj/Shmily/blob/master/LearnGroovy/src/main/lib/ImpalaJDBC41.jar" target="_blank" rel="noopener">Donwload ImpalaJDBC41.jar</a></p>
<pre><code class="shell">cd seatunnel-1.5.6/
mkdir -p plugins/my_plugins/lib
cd plugins/my_plugins/lib
cp /hadoop/bigdata/common/lib/ImpalaJDBC41.jar .</code></pre>
<p>执行抽取任务：</p>
<pre><code class="shell">/opt/seatunnel-1.5.5/bin/start-seatunnel.sh --master yarn --deploy-mode cluster --config /opt/seatunnel-1.5.5/config/impala2ch.batch.conf</code></pre>
<p>此时可以正常抽取数据了，但通过观察程序WebUI发现无论给了多少ExecutorCore，只有一个Task，这样低的并行度会极大影响数据抽取效率，所以需要在配置上做改进：<br>参考<a href="https://interestinglab.github.io/seatunnel-docs/#/zh-cn/v1/configuration/input-plugins/Jdbc?id=jdbc-string" target="_blank" rel="noopener"><strong>SeaTunnel-Spark-jdbc-string</strong></a> 得知SeaTunnel支持SparkJDBC的所有参数:<a href="https://spark.apache.org/docs/2.4.0/sql-data-sources-jdbc.html" target="_blank" rel="noopener"><strong>spark-sql-data-sources-jdbc</strong></a></p>
<p>配置修改思路是将原来的只有一个并行度增加到多个并行度<br>所以使用partitionColumn, lowerBound, upperBound和numPartitions这四个参数进行调优，注意要对分区字段值数据有一定了解，选择合适的分区字段和lowerBound, upperBound很关键。当然这样并行加载数据源也将并行初始化多个连接，Spark源码中提醒到不要并行度过大，否则容易把外部存储搞垮。<br><img src="https://blog-images-1257889704.cos.ap-chengdu.myqcloud.com/BlogImages/SeaTunnel/Seatunnel-03.png" alt="alt ">  </p>
<p>partitionColumn, lowerBound, upperBound和numPartitions这四个参数能决定Spark读取JDBC数据源的并行度及策略，lowerBound是分区字段取值的下限(包含)，upperBound是上限(不包含)，numPatitions是我们希望按照多少分区来加载JDBC。<br>注意第0个分区和最后一个分区加载的数据不被lowerBound, upperBound所限制，仍然会把所有数据加载出来。<br>具体实现逻辑可以看Spark中JdbcRelationProvider和JDBCRelation两个核心类。</p>
<p>根据配置样例<a href="https://interestinglab.github.io/seatunnel-docs/#/zh-cn/v1/configuration/input-plugins/Jdbc?id=example" target="_blank" rel="noopener">SeaTunnel-JDBC-Example</a> 修改配置如下：</p>
<pre><code class="config">spark {
  spark.app.name = &quot;impala-jdbc-2-clickhouse-jdbc&quot;
  spark.executor.instances = 5
  spark.executor.cores = 2
  # 每个excutor内存
  spark.executor.memory = &quot;2g&quot;
}
input {
 jdbc {
     driver = &quot;com.cloudera.impala.jdbc41.Driver&quot;
     url = &quot;jdbc:impala://impalad_ip:21050/default&quot;
     table = &quot;(select * from qjj_test) as source_table&quot;
     # 或者直接写表名也可以table = &quot;qjj_test&quot;
     result_table_name = &quot;impala_table_source&quot;
     user = &quot;&quot;
     password = &quot;&quot;
     jdbc.partitionColumn = &quot;id&quot;
     jdbc.numPartitions = &quot;20&quot;
     jdbc.lowerBound = 0
     jdbc.upperBound = 2000000
 }
}
filter {
}
output {
 clickhouse {
    source_table_name=&quot;impala_table_source&quot;
    host = &quot;ch_jdbc_ip:8123&quot;
    clickhouse.socket_timeout = 50000
    database = &quot;default&quot;
    table = &quot;qjj_test&quot;
    username = &quot;default&quot;
    password = &quot;123456&quot;
    # 每批次写入ClickHouse数据条数
    bulk_size = 20000
 }
}</code></pre>
<p>再次执行，观察WebUI发现并行度已经提高了，写入速度也变快了。</p>
<p>跑到后面发现有发生数据倾斜，可能是因partitionColumn参数设置不合理导致数据倾斜，要注意尽量选择不同范围数据分布均匀的字段作为分区字段，否则极易发生数据倾斜。但通过观察原表数据，发现没有数据在不同范围内分布均匀的字段，所以需要自己造一个分布均匀的字段。可以对字段做MOD(ASCII(SUBSTR(字段名,-1)), 分区数)操作。<br>修改配置如下：</p>
<pre><code class="config">spark {
  spark.app.name = &quot;impala-jdbc-2-clickhouse-jdbc&quot;
  # 提高了分区数 相应的在jdbc允许的jdbc连接数范围内调大executor核数 以更高的并行度跑数据
  spark.executor.instances = 30
  spark.executor.cores = 2
  # 每个excutor内存
  spark.executor.memory = &quot;2g&quot;
}
input {
 jdbc {
     driver = &quot;com.cloudera.impala.jdbc41.Driver&quot;
     url = &quot;jdbc:impala://impalad_ip:21050/default&quot;
     # 注意table的值是交给数据源jdbc去运行的而非Spark，不能使用SparkSQL函数，只能使用数据源支持的函数  次数将数据打散成300个区  可以使用不同的数据打散方式 最好先groupby测一下是否将数据均匀打散
     table = &quot;(select id,name,(cast(rand() * 300 as int)) as spark_partition_column from qjj_test) as source_table&quot;
     result_table_name = &quot;impala_table_source&quot;
     user = &quot;&quot;
     password = &quot;&quot;
     jdbc.partitionColumn = &quot;spark_partition_column&quot;
     jdbc.numPartitions = &quot;300&quot;
     jdbc.lowerBound = 0
     jdbc.upperBound = 300
 }
}
filter {
  sql {
       # 上面处理后多出来个字段，忽略掉该字段
       sql = &quot;select id,name from impala_table_source&quot;
  }
}
output {
 clickhouse {
    # source_table_name=&quot;impala_table_source&quot;
    host = &quot;ch_jdbc_ip:8123&quot;
    clickhouse.socket_timeout = 50000
    database = &quot;default&quot;
    table = &quot;qjj_test&quot;
    username = &quot;default&quot;
    password = &quot;123456&quot;
    # 每批次写入ClickHouse数据条数
    bulk_size = 20000
 }
}</code></pre>
<p>对于使用Impala JDBC进行数据抽取的情况，查询的并行度需要根据服务器数量和资源情况设置，连接并行度不应过大，Impalad对单池内存大小有限制。并行度太高会报如下错误：</p>
<pre><code class="error">Caused by: java.sql.SQLException: [Cloudera][ImpalaJDBCDriver](500051) ERROR processing query/statement. Error Code: 0, SQL state: ExecQueryFInstances rpc query_id=42464c52f2e2c5dc:fe9ecfe800000000 failed: Failed to get minimum memory reservation of 272.00 MB on daemon data02.smycluster.sa:22000 for query 42464c52f2e2c5dc:fe9ecfe800000000 due to following error: Failed to increase reservation by 272.00 MB because it would exceed the applicable reservation limit for the &quot;Process&quot; ReservationTracker: reservation_limit=39.10 GB reservation=38.91 GB used_reservation=0 child_reservations=38.91 GB
The top 5 queries that allocated memory under this tracker are:
Query(8a4d40e3a6968443:7ae87ca100000000): Reservation=28.67 GB ReservationLimit=36.80 GB OtherMemory=21.24 MB Total=28.69 GB Peak=28.79 GB
Query(bb4dc7b08c698bc3:f4036eb000000000): Reservation=1.06 GB ReservationLimit=36.80 GB OtherMemory=93.62 MB Total=1.15 GB Peak=2.39 GB
Query(8a41df2c931faaec:ae30808c00000000): Reservation=1.06 GB ReservationLimit=36.80 GB OtherMemory=68.75 MB Total=1.13 GB Peak=1.37 GB
Query(604eddfbd1fd2de5:b7493a7400000000): Reservation=1.06 GB ReservationLimit=36.80 GB OtherMemory=66.37 MB Total=1.13 GB Peak=1.38 GB
Query(4c4ff283b5e12385:903c399c00000000): Reservation=1.06 GB ReservationLimit=36.80 GB OtherMemory=47.71 MB Total=1.11 GB Peak=1.39 GB
Memory is likely oversubscribed. Reducing query concurrency or configuring admission control may help avoid this error.</code></pre>
<p>在海量数据且资源配置不佳的情况下，使用Impala JDBC导出数据并不是很好的选择，Impala本身不适合跑批，跑批稳定性差，无容错机制。<br>对于这样的场景可以将Impala表数据导出成Parquet文件，再Load到ClickHouse。也可以导出Parquet表到HDFS，再使用ClickHouse映射HDFS引擎表从而获取数据。</p>
<pre><code class="error">org.apache.kudu.client.NonRecoverableException: Scanner 10150be3c0b944829d4eea1bc2251e24 not found (it may have expired)</code></pre>
<p>原因及解决：通常我们需要知道，当带宽占用接近总带宽的90%时，丢包情形就会发生。网络策略有问题或者带宽过低，对带宽做了限制，都会导致这样的问题，取消限制即可。若担心带宽问题，可以适当降低并行度抽取。</p>
<p>2022.1月-SeaTunnel正式进入Apache孵化器，我认为这是个比较优秀的项目，是个低代码实现数据抽取的高效平台，有兴趣可以多关注这个项目。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考:"></a>参考:</h2><p><a href="https://github.com/InterestingLab/seatunnel" target="_blank" rel="noopener">SeaTunnel-github</a><br><a href="https://interestinglab.github.io/seatunnel-docs/#/zh-cn/v1/configuration/base" target="_blank" rel="noopener">SeaTunnel-docs-configuration</a><br><a href="https://blog.csdn.net/qq_40105563/article/details/119247369" target="_blank" rel="noopener">使用WaterDrop将Kudu数据抽取到Clickhouse</a></p>

</div>


    <div class="post-guide">
        <div class="item left">
            
              <a href="/ee1c2df4/">
                  <i class="fa fa-angle-left" aria-hidden="true"></i>
                  Kyuubi原理与替代SparkThriftServer实践
              </a>
            
        </div>
        <div class="item right">
            
              <a href="/3f34ebe3/">
                
                <i class="fa fa-angle-right" aria-hidden="true"></i>
              </a>
            
        </div>
    </div>



	<div id="vcomments"></div>


<script>
	
		// 评论
		new Valine({
			av: AV,
			el: '#vcomments',
			notify: false,
			verify: false,
			path: window.location.pathname,
			appId: 'zUyVEaHo59RUUwiPTChPEeBj-gzGzoHsz',
			appKey: 'xIEyTcrkTuJLz6ewPbpTj8mz',
			placeholder: '欢迎评论...',
			avatar: 'retro',
			recordIP: false
		})
	
	
</script>
	</div>
	<div id="footer">
	<p>
	©2020-<span id="footerYear"></span> 
	<a href="/">佳境Shmily</a> 
	
	
		|
		<span id="busuanzi_container_site_pv">
			pv
			<span id="busuanzi_value_site_pv"></span>
		</span>
		|
		<span id="busuanzi_container_site_uv"> 
			uv
			<span id="busuanzi_value_site_uv"></span>
		</span>
	
	<br>
	Theme <a href="//github.com/wujun234/hexo-theme-tree" target="_blank">Tree</a>
	by <a href="//github.com/wujun234" target="_blank">WuJun</a>
	Powered by <a href="//hexo.io" target="_blank">Hexo</a>
	</p>
</div>
<script type="text/javascript"> 
	document.getElementById('footerYear').innerHTML = new Date().getFullYear() + '';
</script>
	<button id="totop-toggle" class="toggle"><i class="fa fa-angle-double-up" aria-hidden="true"></i></button>
</body>
</html>