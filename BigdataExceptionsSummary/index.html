<!DOCTYPE html>
<html lang="en">

<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8">
	<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
	
	<!-- title -->
	
	<title>
	
		大数据平台常见异常处理汇总 | 
	 
	佳境的技术专区
	</title>
	
	<!-- keywords,description -->
	
		<meta name="keywords" content="异常分析处理," />
	 
		<meta name="description" content="总结平台维护与异常处理过程" />
	

	<!-- favicon -->
	
	<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/Shmilyqjj/Shmilytech.github.io@hexo/themes/hexo-theme-tree/source/favicon.ico">
	
  

	
<link rel="stylesheet" href="/css/main.css">

	
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css">

	
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/styles/darcula.min.css">


	
<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/highlight.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/npm/jquery-pjax@2.0.1/jquery.pjax.min.js"></script>

	
<script src="/js/main.js"></script>

	
		
<script src="https://cdn.jsdelivr.net/npm/leancloud-storage/dist/av-min.js"></script>

		
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>

	
	
		<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	
<meta name="generator" content="Hexo 4.2.1"></head>

<body>
	<header id="header">
    <a id="title" href="/" class="logo">佳境的技术专区</a>
	<ul id="menu">
		<li class="menu-item">
			<a href="https://shmily-qjj.top/" class="menu-item-link" target="_self">
				<input type="image" src="https://cdn.jsdelivr.net/gh/Shmilyqjj/Shmily-Web@master/cdn_sources/img/logo/gotoMain.png" width="90" height="35" alt="回到主站"/>
			</a>
		</li>
		<li class="menu-item">
			<a href="https://shmily-qjj.top/about/" target="_blank" rel="noopener" class="menu-item-link">
				<input type="image" src="https://cdn.jsdelivr.net/gh/Shmilyqjj/Shmily-Web@master/cdn_sources/img/logo/aboutMe.png" width="90" height="35" alt="关于我"/>
			</a>
		</li>

		<li class="menu-item">
			<a href="https://github.com/Shmilyqjj" class="menu-item-link" target="_blank">
<!--				<i class="fa fa-github fa-2x"></i>-->
				<input type="image" src="https://cdn.jsdelivr.net/gh/Shmilyqjj/Shmily-Web@master/cdn_sources/img/logo/myGithub.png" width="90" height="35" alt="关于我"/>
			</a>
		</li>
	</ul>
</header>

	
<div id="sidebar">
	<button id="sidebar-toggle" class="toggle" ><i class="fa fa-arrow-right " aria-hidden="true"></i></button>
	
	<div id="site-toc">
		<input id="search-input" class="search-input" type="text" placeholder="search...">
		<div id="tree">
			

			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										01数据结构与算法
									</a>
									
							<ul>
								<li class="file">
									<a href="/6a894937/">
										基础算法学习
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										02计算机网络
									</a>
									
							<ul>
								<li class="file">
									<a href="/4a17b156/">
										hello-world
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										03操作体统
									</a>
									
							<ul>
								<li class="file">
									<a href="/3f34ebe3/">
										基于Manjaro KDE版打造美观舒适开发环境
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										04编程语言
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										Java
									</a>
									
							<ul>
								<li class="file">
									<a href="/508b5c7/">
										系统学习JVM
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/6f97dc89/">
										线程进程与锁
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										Python
									</a>
									
							<ul>
								<li class="file">
									<a href="/2ed52290/">
										高效运行Python方案
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										05数据库
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										MySQL
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										原理深入
									</a>
									
							<ul>
								<li class="file">
									<a href="/7c15e85/">
										MySQL索引原理深入
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/1f7eb1b3/">
										数据库事务ACID理解
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										操作
									</a>
									
							<ul>
								<li class="file">
									<a href="/3c26421b/">
										Mysql Event Scheduler
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/96009187/">
										浅谈group by与distinct去重
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										06分布式
									</a>
									
							<ul>
								<li class="file">
									<a href="/4a17b156/">
										hello-world
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										07容器
									</a>
									
							<ul>
								<li class="file">
									<a href="/4a17b156/">
										hello-world
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										08大数据
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										交互查询
									</a>
									
							<ul>
								<li class="file">
									<a href="/5f26355/">
										Apache Kudu总结
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/1ae37d82/">
										Impala-基于内存的高效SQL交互查询引擎
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/4c197c46/">
										Presto-基于内存的高效SQL交互查询引擎
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										其他
									</a>
									
							<ul>
								<li class="file">
									<a href="/4b21953d/">
										分享我的技术调研流程
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/39595/">
										记一次参加QCon全球软件开发大会
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="file active">
									<a href="/BigdataExceptionsSummary/">
										大数据平台常见异常处理汇总
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										平台运维
									</a>
									
							<ul>
								<li class="file">
									<a href="/38328/">
										CentOS7安装CDH6全程记录
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										数据仓库
									</a>
									
							<ul>
								<li class="file">
									<a href="/84534d72/">
										SeaTunnel开源数据同步平台
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/26078/">
										Sqoop学习笔记
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										数据可视化
									</a>
									
							<ul>
								<li class="file">
									<a href="/174820fd/">
										Apache Zeppelin初探
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										数据安全
									</a>
									
							<ul>
								<li class="file">
									<a href="/f5da73a2/">
										大数据脱敏方案调研
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/4cf161e5/">
										实现基于Spark的数据脱敏
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										数据湖
									</a>
									
							<ul>
								<li class="file">
									<a href="/44511/">
										Alluxio-基于内存的虚拟分布式存储系统
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										离线计算
									</a>
									
							<ul>
								<li class="file">
									<a href="/7fbbfd34/">
										Hive3.x新特性
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/pyspark_pandas/">
										使用PySpark优化Pandas
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="file">
									<a href="/welcome/">
										欢迎
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
		</div>
	</div>
</div>

	<!-- 引入正文 -->
	<div id="content">
		<h1 id="article-title">

	大数据平台常见异常处理汇总
</h1>
<div class="article-meta">
	
	<span>佳境Shmily</span>
	<span>2021-01-30 12:50:00</span>
    
		<div id="article-categories">
            
                
                    <span>
                        <i class="fa fa-folder" aria-hidden="true"></i>
                        <a href="/categories/技术/">技术</a>
						
                    </span>
                
            
		</div>
    
</div>

<div id="article-content">
	<h1 id="大数据平台常见异常处理汇总"><a href="#大数据平台常见异常处理汇总" class="headerlink" title="大数据平台常见异常处理汇总"></a>大数据平台常见异常处理汇总</h1><p>本博客记录工作中遇到的，大数据相关各个组件的异常处理过程，养成良好的问题归纳总结习惯，累积问题解决经验与思路。</p>
<h2 id="Spark相关"><a href="#Spark相关" class="headerlink" title="Spark相关"></a>Spark相关</h2><ol>
<li><p>Shuffle异常导致任务失败<br>报错：<font size="3" color="red">org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1</font><br>原因：<br>shuffle分为shuffle write和shuffle read两部分。<br>shuffle write的分区数由上一阶段的RDD分区数控制，shuffle read的分区数则是由Spark提供的一些参数控制。<br>shuffle write可以简单理解为类似于saveAsLocalDiskFile的操作，将计算的中间结果按某种规则临时放到各个executor所在的本地磁盘上。<br>shuffle read的时候数据的分区数则是由spark提供的一些参数控制。可以想到的是，如果这个参数值设置的很小，同时shuffle read的量很大，那么将会导致一个task需要处理的数据非常大。结果导致JVM crash，从而导致取shuffle数据失败，同时executor也丢失了，看到Failed to connect to host的错误，也就是executor lost的意思。有时候即使不会导致JVM crash也会造成长时间的gc。<br>解决思路：减少shuffle的数据量和增加处理shuffle数据的分区数<br>①spark.sql.shuffle.partitions控制分区数，默认为200，根据shuffle的量以及计算的复杂度提高这个值 shuffle并行度<br>②提高spark.executor.memory<br>③map side join或是broadcast join来规避shuffle的产生<br>④分析数据倾斜 解决数据倾斜<br>⑤增加失败的重试次数和重试的时间间隔<br>通过spark.shuffle.io.maxRetries控制重试次数，默认是3，可适当增加，例如10。<br>通过spark.shuffle.io.retryWait控制重试的时间间隔，默认是5s，可适当增加，例如10s。<br>⑥类似RemoteShuffleService的服务，解决Shuffle单台机器IO瓶颈，记录Shuffle状态，大批量提升Shuffle效率和稳定性。</p>
</li>
<li><p>SparkSQL报awaitResult异常<br>报错：<font size="3" color="red">org.apache.spark.SparkException: Exception thrown in awaitResult</font><br>原因：广播数据超时<br>解决：spark.sql.broadcastTimeout=1200 默认大小300</p>
</li>
<li><p>HiveOnSpark不能创建SparkClient及Return Code 1异常<br>报错：<font size="3" color="red">FAILED：SemanticException Failed to get a spark session: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to create spark client.</font><br><font size="3" color="red">Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask</font><br>原因：以上报错证明初始化Spark失败，而以前不会失败，所以大概率是资源问题而不是代码问题，查看Yarn队列发现所提交的队列已满且已超过能申请资源的上限（虚线部分），故任务启动失败<br>解决：CM界面-&gt;群集-&gt;动态资源池配置-&gt;提高队列的资源权重（上限也会响应提高）-&gt;刷新动态资源池配置</p>
</li>
<li><p>ExecutorLost、Task Lost<br>报错：<br>1.[<font size="3" color="red">executor lost</font>] WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, aa.local): ExecutorLostFailure (executor lost)<br>2.[<font size="3" color="red">task lost</font>] WARN TaskSetManager: Lost task 69.2 in stage 7.0 (TID 1145, 192.168.47.217): java.io.IOException: Connection from /xx.xxx.xx.xxx:xxxxx closed<br>3.[<font size="3" color="red">timeout</font>] java.util.concurrent.TimeoutException: Futures timed out after [120 second<br>ERROR TransportChannelHandler: Connection to /xxx.xxx.xx.xxx:xxxxx has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong<br>原因：节点资源不足、网络延迟波动、GC导致Executor运行慢等原因<br>解决：①spark.network.timeout的值（默认为120s,配置所有网络传输的延时），根据情况改成300(5min)或更高 ②分别增加各类超时参数<br>spark.core.connection.ack.wait.timeout<br>spark.akka.timeout<br>spark.storage.blockManagerSlaveTimeoutMs<br>spark.shuffle.io.connectionTimeout<br>spark.rpc.askTimeout or spark.rpc.lookupTimeout</p>
</li>
<li><p>SparkThriftServer无法链接jdbc,后台报错Task has been rejected by ExecutorService<br>报错：<br>2021-08-01 02:26:32.028 WARN  [Thread-43] [10.139.53.62] org.apache.thrift.server.TThreadPoolServer.serve(TThreadPoolServer.java:185) :- Task has been rejected by ExecutorService 9 times till timedout, reason: java.util.concurrent.RejectedExecutionException: Task org.apache.thrift.server.TThreadPoolServer$WorkerProcess@6b4f8abf rejected from java.util.concurrent.ThreadPoolExecutor@48ca75d3[Running, pool size = 500, active threads = 500, queued tasks = 0, completed tasks = 917]<br>原因：每次连接都是一个socket连接，都会提交一个Runnable对象到ExecutorService线程池，线程池默认最大500,连接不使用且未关闭就会占用一个线程，占满就无法再连接<br>解决：hive-site.xml调整：<br>hive.server2.session.check.interval 6h-&gt;1h<br>hive.server2.idle.session.timeout 7d-&gt;1d<br>hive.server2.thrift.max.worker.threads (500-&gt;800) (根据用户量大概估，一个用户可能多个Socket/JDBC连接)<br>目的：避免因socket连接对象线程池被占满导致无法连接jdbc</p>
<pre><code class="xml">&lt;property&gt;
 &lt;name&gt;hive.server2.session.check.interval&lt;/name&gt;
 &lt;value&gt;1h&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;hive.server2.idle.session.timeout&lt;/name&gt;
 &lt;value&gt;1d&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;hive.server2.thrift.max.worker.threads&lt;/name&gt;
 &lt;value&gt;800&lt;/value&gt;
&lt;/property&gt;</code></pre>
<p>注意：hive.server2.session.check.interval &lt; hive.server2.idle.operation.timeout &lt; hive.server2.idle.session.timeout</p>
</li>
<li><p>Spark SQL解析错误unresolved object, tree: ArrayBuffer(a).*</p>
<pre><code class="error">Error executing query, currentState RUNNING, 
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to toAttribute on unresolved object, tree: ArrayBuffer(a).*
     at org.apache.spark.sql.catalyst.analysis.Star.toAttribute(unresolved.scala:245)
     at org.apache.spark.sql.catalyst.plans.logical.Project$$anonfun$output$1.apply(basicLogicalOperators.scala:52)
     at org.apache.spark.sql.catalyst.plans.logical.Project$$anonfun$output$1.apply(basicLogicalOperators.scala:52)
     at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
     at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
     at scala.collection.immutable.List.foreach(List.scala:392)
     at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
     at scala.collection.immutable.List.map(List.scala:296)
......</code></pre>
<p>SQL:</p>
<pre><code class="sql">select xxx from table_a a
LEFT JOIN table_b b ON a.cust_no = b.cust_no
LEFT JOIN table_e e ON a.cust_no = e.cust_no
LEFT JOIN table_f f ON a.cust_no = f.cust_no
LEFT JOIN table_g g ON a.cust_no = g.cust_no
.....
LEFT JOIN table_n n ON a.cust_no = n.cust_no;</code></pre>
<p>原因：Spark2.4版本Catalyse模块的一个Bug，table_b<del>table_n中但凡有一张表不存在都会抛该异常，而非NoSuchTableException。底层表不存在，就无法将“unresolved object”转换为“resolved object”，于是报了该错误。<br>解决：确保table_b</del>table_n中每个表都存在即可解决。</p>
</li>
</ol>
<h2 id="HDFS相关"><a href="#HDFS相关" class="headerlink" title="HDFS相关"></a>HDFS相关</h2><ol>
<li><p>数据块丢失且命令无法修复<br>起因：多张表查询发现如下报错，提示块丢失<br><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Bigdata-Problems/HDFS/HDFS-Problems-01.png" alt="alt"><br>分析：CM界面看HDFS丢失块，发现有2500多，大批量块丢失可能的原因：<br> 1.DataNode与NameNode未通信，DataNode进程未启动<br> 2.DataNode数据磁盘损坏，数据丢失<br> 3.一个文件的全部副本丢失<br>解决过程：<br>尝试修复丢失块：hdfs debug recoverLease -path <path-of-the-file> -retries <retry times><br>显示修复成功，但使用hadoop fs -text <file_name> 还是报MissingBlock无法读取<br>使用fsck检测坏块 hdfs fsck /user/hive/warehouse<br><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Bigdata-Problems/HDFS/HDFS-Problems-02.png" alt="alt"><br>发现绝大多数block名称都带有172.xxx.xxx.11 定位到可能是172.xxx.xxx.11节点的DataNode可能存在问题<br>通过CM日志和机器上进程状态判断172.xxx.xxx.11的DataNode已与NameNode保持心跳，运行正常，进而怀疑磁盘坏了（概率太小）<br>查看CM配置和机器磁盘，发现少配置了些硬盘路径，原因是在配置新节点磁盘路径时误修改整个配置组的磁盘路径，导致该配置组中所有DataNode缺少磁盘，进而出现块丢失且无法修复的问题。<br>解决：还原磁盘配置，滚动重启该配置组中的DataNode，将不同机器配置分成多个配置组，重新修改配置<br>重启过程中丢失块数一直在减少：<br><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Bigdata-Problems/HDFS/HDFS-Problems-03.png" alt="alt"><br>最终恢复正常<br>总结：<br>1.CM上修改配置一定要慎重，注意修改配置组中某台节点的配置会影响整个配置组中所有节点的配置<br>2.CM显示DataNode重启成功只是进程启动成功，但日志出现“Total time to add all replicas to map”字眼才是真正完成启动<br>3.<a href="https://hdfs-site/dfshealth.html#tab-overview" target="_blank" rel="noopener">https://hdfs-site/dfshealth.html#tab-overview</a> 从HDFS WebUI获取更多信息（丢失块的信息一目了然）</p>
</li>
<li><p>Win开发Hadoop环境winutils<br>错误：<font size="3" color="red">Could not locate executable null\bin\winutils.exe in the Hadoop binaries</font><br>解决：将winutils.exe放在HADOOP_HOME\bin下，然后代码里System.setProperty(“hadoop.home.dir”, “D:\Programming\Env\Hadoop\hadoop-2.7.2\“)或设置环境变量HADOOP_HOME和PATH后重启电脑<br>winutils.exe下载地址：<a href="https://github.com/steveloughran/winutils" target="_blank" rel="noopener">winutils-master</a></p>
</li>
<li><p>Namenode is not formatted<br>错误Namenode is not formatted<br>分析：一般新部署的HDFS才会提示需要格式化(hadoop namenode -format)，而生产环境Namenode一旦有异常，也可能出现这样的问题，但我不能格式化生产环境啊！<br>NN日志如下：<br><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Bigdata-Problems/HDFS/HDFS-Problems-04.png" alt="alt"><br>可知识由于加载fsimage异常，去NameNode的存储路径/dfs/nn/发现没有文件，而其他节点/dfs/nn目录下有current目录，current目录下有fsimage。所以可以判定该Namenode无法启动就是因为fsimage丢失<br>解决：scp这个current目录，权限与之前的一致（一般为hdfs:hdfs权限）</p>
</li>
<li><p>修复HDFS坏块<br>查看坏块可通过</p>
<pre><code class="shell">hdfs fsck /
hadoop dfsadmin -report</code></pre>
<p>修复坏块除了上面讲过的hdfs debug recoverLease -path <path-of-the-file> -retries <retry times>命令外，还可以通过hadoop fs -setrep -R 3 / 命令，设置副本，副本小于3的情况会自动恢复三副本，修复后还有坏块那就是一个副本都没有了，只能丢了。</p>
</li>
<li><p>两个NN均为Standby，重启后过一会又都Standby<br>NN均报错：<br>java.util.concurrent.ExecutionException: java.io.IOException: Cannot find any valid remote NN to service request!<br>Caused by: java.io.IOException: Cannot find any valid remote NN to service request!<br>Exception from remote name node RemoteNameNodeInfo [nnId=namenode37, ipcAddress=xxxx, httpAddress=xxxx], try next.<br>org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category JOURNAL is not supported in state standby. Visit <a href="https://s.apache.org/sbnn-error" target="_blank" rel="noopener">https://s.apache.org/sbnn-error</a><br>解决：强制切换主备sudo -uhdfs hdfs haadmin -transitionToActive –forcemanual namenode37（nnId在hdfs-site有配） 切换后重启Failover Controller或ZKFC。</p>
</li>
<li><p>基于Sentry认证的HDFS集群NN启动异常<br>NN进程启动成功但是一直处于忙碌状态，查看NN的日志</p>
<pre><code class="log">晚上9点53:24.664分 INFO SentryAuthorizationInfo Refresh interval [500]ms, retry wait [30000]
晚上9点53:24.664分 INFO SentryAuthorizationInfo stale threshold [60000]ms
晚上9点53:24.669分 INFO HMSPaths HMSPaths:[/user/hive/warehouse, /user/hive/xxxx] Initialized
晚上9点53:24.681分 INFO SentryTransportPool Creating pool for xxx:8038,xxx:8038 with default port 8038
晚上9点53:24.683分 INFO SentryTransportPool Adding endpoint xxx:8038
晚上9点53:24.683分 INFO SentryTransportPool Adding endpoint xxx:8038
晚上9点53:24.683分 INFO SentryTransportPool Connection pooling is disabled
日志卡在这不继续了，卡在这</code></pre>
<p>与此同时两个SentryServer有一台发生OOM，不稳定<br>原因：由于基于Sentry认证，HDFS的NN在启动时，Sentry会全量收集HMSPaths下的ACL信息，会驻留在Sentry内存中，如果Sentry内存不足容易OOM，需要通过增加堆内存的方式解决。两个SentryServer虽然可以负载均衡高可用，当一个SentryServer宕掉请求才会转移到另一个节点，而OOM时，请求会一直卡住，不会自动转移，导致NN启动异常。<br>解决：增加Sentry堆内存，不盲目加，根据当前Hive服务器数、库数、表数、分区数、列数及视图数进行评估，具体评估标准如下：<br><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Bigdata-Problems/HDFS/HDFS-Problems-05.png" alt="alt"><br>如图推算：每百万个Hive对象（库、表、分区数）需要配置2.25G的Sentry最大堆内存</p>
</li>
<li><p>distcp数据拷贝报错<br>distcp报错file might have been written to during copy, consider enabling HDFS Snapshots to avoid this error.<br>hdfs-site.xml里增加dfs.namenode.snapshot.capture.openfiles 值为true 开启Immutable Snapshot，保证快照目录里所有文件的状态都是关闭的，文件大小都是创建快照时的状态，解决同步hdfs数据报错文件不存在，或者报不能复制一个打开的或正在写入的文件这些问题。</p>
</li>
<li><p>NameNode启动后无法RPC和切换状态并一直FullGC，几乎80%以上时间都在FullGC</p>
<pre><code class="log">2021-07-14 09:38:35,803 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 47892ms
GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=48186ms
2021-07-14 09:39:25,780 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 47976ms
GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=48428ms
2021-07-14 09:40:15,526 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 47744ms
GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=47840ms
2021-07-14 09:41:04,652 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 47126ms
GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=47437ms
2021-07-14 09:41:55,106 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 48452ms
GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=48767ms
2021-07-14 09:42:48,440 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 51332ms
GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=51719ms
2021-07-14 09:43:40,527 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 50086ms
GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=50450ms
2021-07-14 09:44:34,750 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 52222ms
GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=52462ms
2021-07-14 09:45:30,759 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 53999ms
GC pool &#39;ConcurrentMarkSweep&#39; had collection(s): count=1 time=54179ms</code></pre>
<p>原因：加载FSImage到堆内存的过程中由于堆内存不足导致无法启动。<br>解决：</p>
</li>
<li><p>增加NameNode堆内存，启动后查看HDFS WebUI-&gt;Overview-&gt;Summary查看主NN堆内存使用情况和目前Blocks数量，对NN内存需求重新做评估，调整合适的堆大小。根据Block数量和增量精细化计算NN的堆内存，避免再次OOM。</p>
</li>
<li><p>如果可能，可以减小HDFS垃圾回收时间。</p>
</li>
<li><p>查看HDFS WebUI-&gt;Snaoshot栏，删除无用的快照，避免快照变化文件占用大量空间和Blocks。</p>
</li>
<li><p>HDFS DataNode磁盘切换<br>停止DN、JN -&gt; 将/dfs目录移至目标磁盘，并修改hdfs对应的磁盘为目标路径（对应节点的dfs.journalnode.edits.dir和dfs.datanode.data.dir） -&gt; 启动DN、JN</p>
</li>
<li><p>HDFS远程Kerberos客户端连接非Kerberos集群报错：<br>hdfs dfs -ls hdfs://remote_nn_ip:8020/user/hive/warehouse/</p>
<pre><code class="text">ls: Failed on local exception: java.io.IOException: Server asks us to fall back to SIMPLE auth, but this client is configured to only allow secure connections.; Host Details : local host is: &quot;client_host/client_ip&quot;; destination host is: &quot;remote_nn_ip&quot;:8020; </code></pre>
<p>原因:本地客户端采用Kerberos认证，但远程Server未开启Kerberos认证，远程为SIMPLE认证<br>解决:<br>设置ipc.client.fallback-to-simple-auth-allowed=true参数即可 参数加在dfs后面 如下</p>
<pre><code class="shell">hdfs dfs -D ipc.client.fallback-to-simple-auth-allowed=true -ls hdfs://remote_nn_ip:8020/user/hive/warehouse/</code></pre>
</li>
</ol>
<h2 id="Hive相关"><a href="#Hive相关" class="headerlink" title="Hive相关"></a>Hive相关</h2><ol>
<li><p>HiveMetaStore状态不良导DDLSQL耗时200s以上<br>HMS进程报错：<font size="3" color="red">hive metastore server Failed to sync requested HMS notifications up to the event ID xxx</font><br>原因分析：查看sentry异常CounterWait源码发现传递的id比currentid大导致一直等待超时，超时时间默认为200s（sentry.notification.sync.timeout.ms）。<br>开启了hdfs-sentry acl同步后，hdfs，sentry，HMS三者间权限同步的消息处理。当突然大批量的目录权限消息需要处理，后台线程处理不过来，消息积压滞后就会出现这个异常。这个异常不影响集群使用，只是会导致create，drop table慢需要等200s，这样等待也是为了追上最新的id。我们这次同时出现了HMS参与同步消息处理的线程被异常退出，导致sentry的sentry_hms_notification_id表数据一直没更新，需要重启HMS。如果积压了太多消息，让它慢慢消费处理需要的时间太长，可能一直追不上，这时可以选择丢掉这些消息。<br>解决：<br> ①可以通过设置sentry.notification.sync.timeout.ms参数调小超时时间，减小等待时间，积压不多的话可以让它自行消费处理掉。<br> ②丢掉未处理的消息，在sentry的sentry_hms_notification_id表中插入一条最大值(等于当前消息的id，从notification_sequence表中获取) ，重启sentry服务。（notification_log 表存储了消息日志信息）</p>
</li>
<li><p>HBase外部表报<font size="3" color="red">Unexpected end-of-input</font><br>起因：使用Hive创建HBase外部表时正常，但使用HBase外部表时报Unexpected end-of-input: was expecting closing<br>分析过程：翻阅源码部分发现异常是在解析外部表创建JSON时发生，于是对比建表语句和Hive元数据库中的TABLE_PARAMS表信息得到原因<br>原因：创建hbase外部表catalog太长导致schema太长，而hive元数据表mysql里的table_params字段param_value字段类型是varchar(4000)建表时由于schema太长，超过4000字符的部分被截断。而使用该表的时候会读元数据，但因为元数据不完整而报错。<br>解决：<br>①分多次建表<br>②改varchar(4000)为longtext然后重建表（影响无法评估，没尝试）</p>
</li>
<li><p>Hive和Spark查询Hive表报java.net.UnknownHostException: nameservice1<br>分析：由于机器HDFS HA配置发生变动，关掉了高可用，所以nameservice变成了ip:8020，报这个错，首先检查了/etc/hadoop/conf/hdfs-site.xml里面无dfs.nameservices配置，证明配置文件没问题。使用hivecli，desc formatted某几张表，发现表的LOCATION均为hdfs://nameservice1/xx 找到了问题的原因，元数据错误，所以需要去Hive元数据库批量修改元数据。<br>解决：连接到Hive metastore元数据库，批量修改LOCATION：<br>update hive.SDS set LOCATION=concat(“hdfs://ip:8020”,substring(LOCATION,20,length(LOCATION)-19)) where LOCATION like “hdfs://nameservice1%”;<br>update hive.DBS set DB_LOCATION_URI=concat(“hdfs://ip:8020”,substring(DB_LOCATION_URI,20,length(DB_LOCATION_URI)-19)) where DB_LOCATION_URI like “hdfs://nameservice1%”;<br>无需重启服务即可生效，问题解决。</p>
</li>
</ol>
<h2 id="Yarn相关"><a href="#Yarn相关" class="headerlink" title="Yarn相关"></a>Yarn相关</h2><ol>
<li><p>应用提交报<font size="3" color="red">Retrying connect to server 0.0.0.0</font><br>原因：应用没有认到yarn-site.xml或者yarn-site.xml配置不正确<br>解决：①指定HADOOP_CONF_DIR ②确认yarn-site.xml</p>
<pre><code class="yarn-site.xml">&lt;property&gt;
  &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
  &lt;value&gt;master:8032&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
  &lt;value&gt;master:8030&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
  &lt;value&gt;master:8031&lt;/value&gt;
&lt;/property&gt;</code></pre>
</li>
<li><p>内存不足Container退出<br>报错：<font size="3" color="red">Diagnostics: Container [pid=91869,containerID=container_e23_1574819880505_43157_01_000001] is running beyond physical memory limits. Current usage: 9.0 GB of 9 GB physical memory used; 12.8 GB of 18.9 GB virtual memory used. Killing container.Dump of the process-tree for container_e23_1574819880505_43157_01_000001</font><br>分析：”physical memory used”为物理内存占用（应用已占满9G），”virtual memory used”为虚拟内存占用，18.9GB是取决于yarn.nodemanager.vmem-pmem-ratio（yarn-site.xml中设置的虚拟内存和物理内存比例，默认2.1），报错是因为物理内存不足，是任务设置的内存少了<br>解决思路：<br>①如果资源充足，增加任务并行度分担任务负载<br>②增大任务可用资源（注意不要超过单台NM可分配上限yarn.scheduler.maximum-allocation-mb的值）<br>③适当增大yarn.nodemanager.vmem-pmem-ratio，适当调高虚拟内存比例<br>④[不建议]取消内存的检查：在yarn-site.xml或者程序中中设置yarn.nodemanager.vmem-check-enabled为false</p>
</li>
</ol>
<h2 id="HBase相关"><a href="#HBase相关" class="headerlink" title="HBase相关"></a>HBase相关</h2><ol>
<li><p>集群新增RS致另一RS异常退出<br><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Bigdata-Problems/HBase/HBase-Problems-01.png" alt="alt"><br>上线的RegionServer会触发Region移动，报错前有Flush操作，因为Region移动前会先Flush Region<br>异常相关源码：<br><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Bigdata-Problems/HBase/HBase-Problems-02.png" alt="alt"><br>Flush Memstore到HFile这个过程未发生异常，但Flush一个Memstore后跟踪Memstore总大小未发生变化，即内存清理失败 超过5次就中止这个RS<br>分析：版本BUG</p>
</li>
<li><p>Couldn’t read snapshot info from hdfs:xx/hbase/.hbase-snapshot/xxx导致Master不可用<br>hbase shell执行list命令报ERROR: org.apache.hadoop.hbase.PleaseHoldException: Master is initializing<br>跟进Master日志看到Couldn’t read snapshot info from hdfs:xx/hbase/.hbase-snapshot/xxx，Master状态不可用<br>分析：由于HDFS的原因导致HBase的Snapshot文件丢失，详细看了一下，其他snapshot目录下有.snapshot以及data.manifest，而这个没有<br>操作：删除这个snapshot目录 重启HMaster<br>修复后scan操作又报Unknown table xxx，而且几乎所有表都不能scan<br>操作：首先想到的就是修复HBase元数据hbase hbck -fixMeta<br>修复后继续scan，报错ERROR: No server address listed in hbase:meta for region t1,,1536659773616.09db0b8b3b7f8cd81dde86c9f1e41306. containing row rowkey001: 1 time…<br>分析：查询hbase:meta表scan ‘hbase:meta’,{LIMIT=&gt;10,FILTER=&gt;”PrefixFilter(‘test_ray’)”}<br><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Bigdata-Problems/HBase/HBase-Problems-03.png" alt="alt"><br>发现表元数据中都没有regionserver的信息，正常情况是这样的：<br><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Bigdata-Problems/HBase/HBase-Problems-04.png" alt="alt"><br>解决：重新对region分区：hbase hbck -fixAssignments  最终服务和数据均恢复正常</p>
</li>
</ol>
<h2 id="Kafka相关"><a href="#Kafka相关" class="headerlink" title="Kafka相关"></a>Kafka相关</h2><ol>
<li>ConsumerRebalanceFailedException异常解决<br>报错：kafka.common.ConsumerRebalanceFailedException: group_xxx-1446432618163-2746a209 can’t rebalance after 5 retries<br>分析：官网给出了异常的相关说明和解决方案<br>consumer rebalancing fails (you will see ConsumerRebalanceFailedException): This is due to conflicts when two consumers are trying to own the same topic partition. The log will show you what caused the conflict (search for “conflict in “).<br>If your consumer subscribes to many topics and your ZK server is busy, this could be caused by consumers not having enough time to see a consistent view of all consumers in the same group. If this is the case, try Increasing rebalance.max.retries and rebalance.backoff.ms.<br>Another reason could be that one of the consumers is hard killed. Other consumers during rebalancing won’t realize that consumer is gone after zookeeper.session.timeout.ms time. In the case, make sure that rebalance.max.retries * rebalance.backoff.ms &gt; zookeeper.session.timeout.ms.<br>简单来说就是同一个topic的同一个分区被多个消费者消费，发生冲突<br>解决：<br>①增加相关topic的partition数<br>②提高kafka的consumer如下两项配置<br>rebalance.backoff.ms=2000<br>rebalance.max.retries=10</li>
</ol>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ol>
<li>Zookeeper日志和快照过大<br>解决1：cd $ZOOKEEPER_HOME;bin/zkCleanup.sh {dataDir} 5;bin/zkCleanup.sh {snapshotDir} 5 并设置成定时调度<br>解决2：从3.4.0开始，zookeeper提供了自动清理snapshot和事务日志的功能，通过配置autopurge.snapRetainCount和autopurge.purgeInterval这两个参数能够实现定时清理了。这两个参数都是在zoo.cfg中配置的：<br> autopurge.purgeInterval 这个参数指定了清理频率，单位是小时，需要填写一个1或更大的整数，默认是0，表示不开启自己清理功能。<br> autopurge.snapRetainCount 这个参数和上面的参数搭配使用，这个参数指定了需要保留的文件数目。默认是保留3个。<br>原理：Zookeeper不会删除旧的快照和日志文件，zkCleanup.sh可以帮助合理清理当前节点的旧的日志和快照文件解决磁盘空间。其中dataDir是特定服务集合的znode存储的永久副本，对znode的所有更改会附加到log日志中，snapshotDir是永久快照。zkCleanup.sh将保留最后一个快照和相关的log日志，清除其他的快照和日志。注意最后一个参数值要大于等于3，日志损坏的情况下保证有3个以上备份。<br>参考:<a href="https://zookeeper.apache.org/doc/r3.4.6/zookeeperAdmin.html#sc_maintenance" target="_blank" rel="noopener">zookeeperAdmin.html#sc_maintenance</a></li>
</ol>

</div>


    <div class="post-guide">
        <div class="item left">
            
              <a href="/4c197c46/">
                  <i class="fa fa-angle-left" aria-hidden="true"></i>
                  交互查询
              </a>
            
        </div>
        <div class="item right">
            
              <a href="/1ae37d82/">
                交互查询
                <i class="fa fa-angle-right" aria-hidden="true"></i>
              </a>
            
        </div>
    </div>



	<div id="vcomments"></div>


<script>
	
		// 评论
		new Valine({
			av: AV,
			el: '#vcomments',
			notify: false,
			verify: false,
			path: window.location.pathname,
			appId: 'zUyVEaHo59RUUwiPTChPEeBj-gzGzoHsz',
			appKey: 'xIEyTcrkTuJLz6ewPbpTj8mz',
			placeholder: '欢迎评论...',
			avatar: 'retro',
			recordIP: false
		})
	
	
</script>
	</div>
	<div id="footer">
	<p>
	©2020-<span id="footerYear"></span> 
	<a href="/">佳境Shmily</a> 
	
	
		|
		<span id="busuanzi_container_site_pv">
			pv
			<span id="busuanzi_value_site_pv"></span>
		</span>
		|
		<span id="busuanzi_container_site_uv"> 
			uv
			<span id="busuanzi_value_site_uv"></span>
		</span>
	
	<br>
	Theme <a href="//github.com/wujun234/hexo-theme-tree" target="_blank">Tree</a>
	by <a href="//github.com/wujun234" target="_blank">WuJun</a>
	Powered by <a href="//hexo.io" target="_blank">Hexo</a>
	</p>
</div>
<script type="text/javascript"> 
	document.getElementById('footerYear').innerHTML = new Date().getFullYear() + '';
</script>
	<button id="totop-toggle" class="toggle"><i class="fa fa-angle-double-up" aria-hidden="true"></i></button>
</body>
</html>