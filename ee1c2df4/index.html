<!DOCTYPE html>
<html lang="en">

<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8">
	<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
	
	<!-- title -->
	
	<title>
	
		Kyuubi原理与替代SparkThriftServer实践-基于CDH6 | 
	 
	佳境的技术专区
	</title>
	
	<!-- keywords,description -->
	
		<meta name="keywords" content="Kyuubi," />
	 
		<meta name="description" content="Kyuubi统一分析引擎代替ThriftServer提供稳定高效、支持多租户、权限管理、动态资源的分析服务。" />
	

	<!-- favicon -->
	
	<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/Shmilyqjj/Shmilytech.github.io@hexo/themes/hexo-theme-tree/source/favicon.ico">
	
  

	
<link rel="stylesheet" href="/css/main.css">

	
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css">

	
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/styles/darcula.min.css">


	
<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/highlight.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/npm/jquery-pjax@2.0.1/jquery.pjax.min.js"></script>

	
<script src="/js/main.js"></script>

	
		
<script src="https://cdn.jsdelivr.net/npm/leancloud-storage/dist/av-min.js"></script>

		
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>

	
	
		<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	
<meta name="generator" content="Hexo 4.2.1"></head>

<body>
	<header id="header">
    <a id="title" href="/" class="logo">佳境的技术专区</a>
	<ul id="menu">
		<li class="menu-item">
			<a href="https://shmily-qjj.top/" class="menu-item-link" target="_self">
				<input type="image" src="https://cdn.jsdelivr.net/gh/Shmilyqjj/Shmily-Web@master/cdn_sources/img/logo/gotoMain.png" width="90" height="35" alt="回到主站"/>
			</a>
		</li>
		<li class="menu-item">
			<a href="https://shmily-qjj.top/about/" target="_blank" rel="noopener" class="menu-item-link">
				<input type="image" src="https://cdn.jsdelivr.net/gh/Shmilyqjj/Shmily-Web@master/cdn_sources/img/logo/aboutMe.png" width="90" height="35" alt="关于我"/>
			</a>
		</li>

		<li class="menu-item">
			<a href="https://github.com/Shmilyqjj" class="menu-item-link" target="_blank">
<!--				<i class="fa fa-github fa-2x"></i>-->
				<input type="image" src="https://cdn.jsdelivr.net/gh/Shmilyqjj/Shmily-Web@master/cdn_sources/img/logo/myGithub.png" width="90" height="35" alt="关于我"/>
			</a>
		</li>
	</ul>
</header>

	
<div id="sidebar">
	<button id="sidebar-toggle" class="toggle" ><i class="fa fa-arrow-right " aria-hidden="true"></i></button>
	
	<div id="site-toc">
		<input id="search-input" class="search-input" type="text" placeholder="search...">
		<div id="tree">
			

			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										01数据结构与算法
									</a>
									
							<ul>
								<li class="file">
									<a href="/6a894937/">
										基础算法学习
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										02计算机网络
									</a>
									
							<ul>
								<li class="file">
									<a href="/4a17b156/">
										hello-world
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										03操作体统
									</a>
									
							<ul>
								<li class="file">
									<a href="/3f34ebe3/">
										基于Manjaro KDE版打造美观舒适开发环境
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										04编程语言
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										Java
									</a>
									
							<ul>
								<li class="file">
									<a href="/508b5c7/">
										系统学习JVM
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/6f97dc89/">
										线程进程与锁
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										Python
									</a>
									
							<ul>
								<li class="file">
									<a href="/2ed52290/">
										高效运行Python方案
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										05数据库
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										MySQL
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										原理深入
									</a>
									
							<ul>
								<li class="file">
									<a href="/7c15e85/">
										MySQL索引原理深入
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/1f7eb1b3/">
										数据库事务ACID理解
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										操作
									</a>
									
							<ul>
								<li class="file">
									<a href="/3c26421b/">
										Mysql Event Scheduler
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/96009187/">
										浅谈group by与distinct去重
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										06分布式
									</a>
									
							<ul>
								<li class="file">
									<a href="/4a17b156/">
										hello-world
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										07容器
									</a>
									
							<ul>
								<li class="file">
									<a href="/4a17b156/">
										hello-world
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										08大数据
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										交互查询
									</a>
									
							<ul>
								<li class="file">
									<a href="/5f26355/">
										Apache Kudu总结
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/1ae37d82/">
										Impala-基于内存的高效SQL交互查询引擎
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/4c197c46/">
										Presto-基于内存的高效SQL交互查询引擎
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										其他
									</a>
									
							<ul>
								<li class="file">
									<a href="/4b21953d/">
										分享我的技术调研流程
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/39595/">
										记一次参加QCon全球软件开发大会
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="file">
									<a href="/BigdataExceptionsSummary/">
										大数据平台常见异常处理汇总
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										平台运维
									</a>
									
							<ul>
								<li class="file">
									<a href="/38328/">
										CentOS7安装CDH6全程记录
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										数据仓库
									</a>
									
							<ul>
								<li class="file">
									<a href="/84534d72/">
										SeaTunnel开源数据同步平台
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/26078/">
										Sqoop学习笔记
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										数据可视化
									</a>
									
							<ul>
								<li class="file">
									<a href="/174820fd/">
										Apache Zeppelin初探
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										数据安全
									</a>
									
							<ul>
								<li class="file">
									<a href="/f5da73a2/">
										大数据脱敏方案调研
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/4cf161e5/">
										实现基于Spark的数据脱敏
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										数据湖
									</a>
									
							<ul>
								<li class="file">
									<a href="/44511/">
										Alluxio-基于内存的虚拟分布式存储系统
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										离线计算
									</a>
									
							<ul>
								<li class="file">
									<a href="/7fbbfd34/">
										Hive3.x新特性
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file active">
									<a href="/ee1c2df4/">
										Kyuubi原理与替代SparkThriftServer实践
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/pyspark_pandas/">
										使用PySpark优化Pandas
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="file">
									<a href="/welcome/">
										欢迎
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
		</div>
	</div>
</div>

	<!-- 引入正文 -->
	<div id="content">
		<h1 id="article-title">

	Kyuubi原理与替代SparkThriftServer实践
</h1>
<div class="article-meta">
	
	<span>佳境Shmily</span>
	<span>2022-04-29 16:39:12</span>
    
		<div id="article-categories">
            
                
                    <span>
                        <i class="fa fa-folder" aria-hidden="true"></i>
                        <a href="/categories/技术/">技术</a>
						
                    </span>
                
            
		</div>
    
</div>

<div id="article-content">
	<h1 id="Kyuubi原理与替代SparkThriftServer实践-基于CDH6"><a href="#Kyuubi原理与替代SparkThriftServer实践-基于CDH6" class="headerlink" title="Kyuubi原理与替代SparkThriftServer实践-基于CDH6"></a>Kyuubi原理与替代SparkThriftServer实践-基于CDH6</h1><p><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Kyuubi/Kyuubi-00.png" alt="alt"></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Spark ThriftServer原生不支持多租户、权限管理、且稳定性一般，即使我们在源码基础上做了很多权限管控、SQL日志审计、数据脱敏以及性能优化，但由于它自身的稳定性和单点问题，仍然会经常造成调度、分析任务的失败。常见的一些问题有：</p>
<ol>
<li>Driver端单点故障导致整个调度失败</li>
<li>单个大Query占用大量并行度，导致后续任务缓慢或持续等待</li>
<li>单个Job发生数据倾斜时，会拖慢该Job的Task所在的Executor，影响其他Job</li>
<li>定期重启以避免Kerberos凭据过期</li>
<li>通过源码二次开发才能实现的用户权限管理、多租户管理</li>
<li>资源申请和释放粒度较粗，导致资源利用浪费或不充分</li>
<li>对不同负载任务启动多个Thrift实例，才能实现粗粒度的资源隔离，实例越多，维护起来越繁琐<br>针对以上痛点，网易贡献了Kyuubi这个项目，非常适合替换掉原有的SparkThriftServer服务，解决以上痛点的同时，还支持了异构计算引擎(Flink、Trino等)。当前(2022.5)该项目还在Apache孵化器进行孵化，在我看来是个比较有前景的项目。</li>
</ol>
<h2 id="Kyuubi是什么"><a href="#Kyuubi是什么" class="headerlink" title="Kyuubi是什么"></a>Kyuubi是什么</h2><p>网易数帆开源的一款支持多租户资源隔离、细粒度的行级、列级权限管理、支持高可用和负载均衡的统一分析引擎，可以通过SQL、Scala完成ETL、数据处理跑批、分析等多种任务负载。<br>Kyuubi的愿景是建立在Apache Spark和Data Lake技术之上，理想的统一数据湖管理平台。支持纯SQL方式处理数据，实现在同统一平台上使用一份数据副本和一个SQL接口，完成ETL、分析、BI……等工作。</p>
<h2 id="Kyuubi对比SparkThriftServer的优势"><a href="#Kyuubi对比SparkThriftServer的优势" class="headerlink" title="Kyuubi对比SparkThriftServer的优势"></a>Kyuubi对比SparkThriftServer的优势</h2><table>
<thead>
<tr>
<th></th>
<th>Kyuubi</th>
<th>SparkThriftServer</th>
</tr>
</thead>
<tbody><tr>
<td>资源隔离</td>
<td>支持资源隔离</td>
<td>STS是单个Application，只能提交到一个Yarn Queue；虽然Spark本身也具有一定资源共享能力——FairScheduler通过设置spark.scheduler.pool资源池优先级来为不同用户分配不同资源，但内存IO和CPU等资源的隔离本身应是资源调度系统Yarn或K8S该做的事儿</td>
</tr>
<tr>
<td>并发和扩展能力</td>
<td>支持无限水平扩展的多客户端并发能力，可自动扩展的查询并发能力，慢SQL影响小</td>
<td>单个STS并发查询能力有限、并发高时就会出现资源紧张，资源抢占，任务等待、卡死，且Driver单点瓶颈明显，慢SQL影响大</td>
</tr>
<tr>
<td>资源伸缩性</td>
<td>两级弹性资源管理（Kyuubi的资源弹性管理支持自动申请和释放Spark实例+Spark应用自身动态资源管理）</td>
<td>Spark自身动态资源管理</td>
</tr>
<tr>
<td>授权控制</td>
<td>支持数据和元数据的访问权限控制，支持基于Ranger细粒度授权，保证数据安全</td>
<td>STS是单用户启动的，只有粗粒度授权，无法保证数据安全</td>
</tr>
<tr>
<td>实例管理</td>
<td>支持连接级别、用户级别、服务级别和组级别的SparkApplication实例申请</td>
<td>单个SparkApplication实例</td>
</tr>
<tr>
<td>执行引擎</td>
<td>Spark、Flink、Trino(Presto)</td>
<td>Spark</td>
</tr>
<tr>
<td>存储引擎</td>
<td>Hive+Kudu+DeltaLake+Azure+Presto</td>
<td>Hive+DeltaLake</td>
</tr>
<tr>
<td>高可用性</td>
<td>原生基于ZK和Yarn的高可用，KyuubiServer本身支持水平扩展高可用</td>
<td>原生不支持，需要手动配置LoadBalancer，但发生切换时视图、hivevar变量、缓存等状态会丢失</td>
</tr>
<tr>
<td>系统架构</td>
<td><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Kyuubi/Kyuubi-01.png" alt="alt"></td>
<td><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Kyuubi/Kyuubi-08.png" alt="alt"></td>
</tr>
</tbody></table>
<h2 id="Kyuubi原理"><a href="#Kyuubi原理" class="headerlink" title="Kyuubi原理"></a>Kyuubi原理</h2><h3 id="Kyuubi架构图"><a href="#Kyuubi架构图" class="headerlink" title="Kyuubi架构图"></a>Kyuubi架构图</h3><p><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Kyuubi/Kyuubi-01.png" alt="alt"><br>在Kyuubi中，客户端的连接是作为<strong>KyuubiSession</strong>来维护的。<br>Kyuubi Session的创建可以分为轻量级和重量级两种情况。大多数会话创建都是轻量级、用户无感知的。唯一的重量级情况是用户的共享域中没有实例化或缓存的SparkContext，这种情况通常发生在用户第一次连接或长时间未连接时。这种一次性创建会话的成本，在多数AdHoc场景下也能接受。</p>
<p>Kyuubi维护SparkContext的方式是松散耦合的，这些SparkContext既可以是本地Client模式创建的，也可以是Yarn、K8S集群上的Cluster模式创建的。高可用模式下，SparkContext也可以由其他机器上的Kyuubi实例创建并共享出来。</p>
<p>Kyuubi可以创建和托管多个SparkContexts实例，它们有自己的生命周期，一定条件下会被自动创建和回收，如果一段时间没有任务负载，资源会全部释放。SparkContext的状态不受Kyuubi进程故障转移的影响。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Kyuubi/Kyuubi-02.png" alt="alt"><br>Kyuubi支持不同共享级别的引擎共享。如果设置了USER级别的share.level，同一用户与Kyuubi建立的多个连接会复用同一个Engine，实现用户级别的资源隔离。</p>
<h3 id="Kyuubi资源隔离共享级别"><a href="#Kyuubi资源隔离共享级别" class="headerlink" title="Kyuubi资源隔离共享级别"></a>Kyuubi资源隔离共享级别</h3><table>
<thead>
<tr>
<th>共享级别</th>
<th>参数</th>
<th>图解</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>CONNECTION</td>
<td>kyuubi.engine.share.level=CONNECTION</td>
<td><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Kyuubi/Kyuubi-09.png" alt="alt"></td>
<td>每个连接都创建一个独立的Engine，连接创建即申请Engine，连接关闭即释放Engine</td>
</tr>
<tr>
<td>USER</td>
<td>kyuubi.engine.share.level=USER</td>
<td><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Kyuubi/Kyuubi-10.png" alt="alt"></td>
<td>同一用户的多个连接共享一个Engine，一个用户对应一个Engine，用户连接关闭后不会立刻释放Engine，在无操作达到TTL后释放Engine</td>
</tr>
<tr>
<td>GROUP</td>
<td>kyuubi.engine.share.level=GROUP</td>
<td><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Kyuubi/Kyuubi-11.png" alt="alt"></td>
<td>属于相同组的所有用户创建的所有连接共享同一个Engine，以组名作为启动Engine的用户名，数据权限按组进行管理，如果组名不存在，共享级别降级为USER，用户组遵循<a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/GroupsMapping.html" target="_blank" rel="noopener">Hadoop Groups Mapping</a>，可以通过配置把不同用户映射到一个组。相比USER级别给每个用户都创建引擎，GROUP级别可以减少引擎实例数，节约资源，但引擎是共享的，同组所有用户都复用这个引擎，访问权限控制若要做到细粒度，则需要结合<a href="https://ranger.apache.org/" target="_blank" rel="noopener">Apache Ranger</a>，资源控制的细粒度需要结合<a href="https://spark.apache.org/docs/latest/job-scheduling.html#fair-scheduler-pools" target="_blank" rel="noopener">SparkFairScheduler</a></td>
</tr>
<tr>
<td>SERVER</td>
<td>kyuubi.engine.share.level=SERVER</td>
<td><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Kyuubi/Kyuubi-12.png" alt="alt"></td>
<td>每个KyuubiServer中的连接共用一个Engine，类似原生ThriftServer的高可用版本</td>
</tr>
<tr>
<td>一个KyuubiServer中可以混用多种隔离级别。</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>比如正常情况下引擎共享级别设置为GROUP，同一个组下的用户只能申请一个引擎；当组里用户太多时，单个引擎也会出现并发瓶颈和资源抢占，针对这种问题，Kyuubi中引入了Subdomain的概念，引擎共享子域（kyuubi.engine.share.level.subdomain）是对引擎资源隔离共享级别的补充，能实现同一个用户、组创建多个引擎。<br>Kyuubi的JDBC连接串模板：jdbc:hive2://kyuubi-server-ip:10009/default;?conf1=val1;conf2=var2;…;confN=varN<br>Kyuubi的JDBC连接串示例：jdbc:hive2://kyuubi-server-ip:10009/default;?spark.driver.memory=5G;spark.app.name=qjj_kyuubi_application<br>Subdomain的使用：</p>
<pre><code class="shell">beeline -u &quot;jdbc:hive2://kyuubi-server-ip:10009/default;?spark.app.name=qjj_kyuubi_sd1;spark.driver.memory=4G;kyuubi.engine.share.level=USER;kyuubi.engine.share.level.subdomain=sd1&quot; -nq00885 -p******
beeline -u &quot;jdbc:hive2://kyuubi-server-ip:10009/default;?spark.app.name=qjj_kyuubi_sd2;spark.driver.memory=2G;kyuubi.engine.share.level=USER;kyuubi.engine.share.level.subdomain=sd2&quot; -nq00885 -p******</code></pre>
<p>可以看到单个用户启动了两个Engine<br><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Kyuubi/Kyuubi-13.png" alt="alt"><br>如果我想创建一个连接复用之前的sd2这个Subdomain，就可以通过以下指定Subdomain的方式进行指定。</p>
<pre><code class="shell">beeline -u &quot;jdbc:hive2://kyuubi-server-ip:10009/default;?kyuubi.engine.share.level=USER;kyuubi.engine.share.level.subdomain=sd2&quot; -nq00885 -p******</code></pre>
<p>参考：<a href="https://kyuubi.apache.org/docs/latest/deployment/engine_share_level.html" target="_blank" rel="noopener">Kyuubi Engine Share Level</a></p>
<h3 id="Kyuubi-HA"><a href="#Kyuubi-HA" class="headerlink" title="Kyuubi HA"></a>Kyuubi HA</h3><p>Kyuubi基于ZK实现高可用和负载均衡：<br>KyuubiServer启动会到ZK注册节点，实现KyuubiServer之间负载均衡和高可用<br>每个用户登录默认是default子域，每个子域注册一个永久节点，子域下面申请的Engine会注册临时节点，将Engine信息写入ZK。此外还通过ZK存放一些用户的锁和租约信息。<br><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Kyuubi/Kyuubi-14.png" alt="alt"></p>
<h3 id="Kyuubi监控"><a href="#Kyuubi监控" class="headerlink" title="Kyuubi监控"></a>Kyuubi监控</h3><p>Kyuubi本身支持监控，配置方法参考：<a href="https://kyuubi.apache.org/docs/latest/monitor/metrics.html" target="_blank" rel="noopener">Monitoring Kyuubi - Server Metrics</a></p>
<h2 id="部署Kyuubi-On-CDH6-3-2"><a href="#部署Kyuubi-On-CDH6-3-2" class="headerlink" title="部署Kyuubi On CDH6.3.2"></a>部署Kyuubi On CDH6.3.2</h2><h3 id="Spark-3-2-2-On-CDH6-3-2编译与部署"><a href="#Spark-3-2-2-On-CDH6-3-2编译与部署" class="headerlink" title="Spark 3.2.2 On CDH6.3.2编译与部署"></a>Spark 3.2.2 On CDH6.3.2编译与部署</h3><p><strong>源码准备</strong></p>
<pre><code class="shell"># Windows下进入wsl环境（Windows的Linux子系统）
wsl
# 下载Spark源码
git clone https://github.com/apache/spark.git
# 切换到spark3.2分支并创建新分支branch-3.2-cdh6.3.2
cd spark
git checkout branch-3.2
git checkout -b branch-3.2-cdh6.3.2</code></pre>
<p><strong>pom修改</strong></p>
<pre><code class="xml">&lt;!-- 增加Cloudera源 --&gt;
&lt;repository&gt;
    &lt;id&gt;cloudera&lt;/id&gt;
    &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;
    &lt;name&gt;Cloudera Repositories&lt;/name&gt;
    &lt;snapshots&gt;
      &lt;enabled&gt;true&lt;/enabled&gt;
    &lt;/snapshots&gt;
&lt;/repository&gt;
&lt;pluginRepository&gt;
    &lt;id&gt;cloudera&lt;/id&gt;
    &lt;name&gt;Cloudera Repositories&lt;/name&gt;
    &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;
&lt;/pluginRepository&gt;
&lt;!-- 增加hadoop3 profile --&gt;
&lt;profile&gt;
    &lt;id&gt;hadoop-3.0&lt;/id&gt;
    &lt;properties&gt;
      &lt;hadoop.version&gt;3.0.0-cdh6.3.2&lt;/hadoop.version&gt;
    &lt;/properties&gt;
&lt;/profile&gt;</code></pre>
<p><strong>编译</strong></p>
<pre><code class="shell"># 转换/r/n为unix系统可正常运行的/n  (CRLF转LF)
sudo apt install dos2unix
dos2unix ./dev/*.sh 
dos2unix ./build/*
# 编译 看能否编译通过：
mvn -Pyarn -Dhadoop.version=3.0.0-cdh6.3.2  -Phadoop-3.0 -Phive-thriftserver -DskipTests clean package --settings &quot;/mnt/d/Applications/apache-maven-3.6.3/conf/settings.xml&quot; -Dmaven.repo.local=&quot;/mnt/e/Maven/Repository&quot;
# 编译&amp;打包 将源码编译为binary包 生成spark-3.2.2-SNAPSHOT-bin-hadoop-3.0.0-cdh6.3.2.tgz安装包：
./dev/make-distribution.sh --name hadoop-3.0.0-cdh6.3.2 --tgz -Phadoop-3.0 -Pyarn -Phive-thriftserver -DskipTests --settings &quot;/mnt/d/Applications/apache-maven-3.6.3/conf/settings.xml&quot; -Dmaven.repo.local=&quot;/mnt/e/Maven/Repository&quot;</code></pre>
<p><strong>部署</strong></p>
<pre><code class="shell">tar -zxvf spark-3.2.2-SNAPSHOT-bin-hadoop-3.0.0-cdh6.3.2.tgz -C ../../
mv spark-3.2.2-SNAPSHOT-bin-hadoop-3.0.0-cdh6.3.2 spark-3.2.2-bin-hadoop-3.0.0-cdh6.3.2
cd spark-3.2.2-bin-hadoop-3.0.0-cdh6.3.2
cd conf 
cp spark-defaults.conf.template spark-defaults.conf
cp spark-env.sh.template spark-env.sh
# 修改spark-defaults.conf (参数生效优先级: SparkConf &gt; spark-submit Flags &gt; spark-defaults.conf)
vim spark-defaults.conf
  ## Java设置
  spark.executorEnv.JAVA_HOME=/usr/java/jdk1.8.0_181
  spark.yarn.appMasterEnv.JAVA_HOME=/usr/java/jdk1.8.0_181
  ## 开启eventLog用于重构历史已完成任务的WebUI    
  spark.eventLog.enabled=true
  spark.eventLog.dir=hdfs:///user/spark/applicationHistory
  spark.eventLog.compress=true
  spark.driver.log.dfsDir=/user/spark/driverLogs  （持久化driver日志的路径）
  spark.driver.log.persistToDfs.enabled=true   （持久化driver日志）
  spark.history.fs.cleaner.enabled=true  (定期自动清理日志目录，默认一天清理一次，清理7天前的日志文件)
  spark.history.fs.logDirectory=hdfs:///user/spark/applicationHistory
  spark.history.ui.port=18080   （访问Spark应用历史记录http://historyServerHost:18080/）
  spark.history.retainedApplications=30   （缓存中保存的应用历史记录个数，超过会将旧的删除，读更早的日志去磁盘读会慢些）
  spark.yarn.historyServer.address=http://cdh101:18080   （Yarn Application页面Tracking URL链接可以直接进入HistoryServer查看日志）
  spark.yarn.historyServer.allowTracking=true  （Yarn Application页面Tracking URL链接可以直接进入HistoryServer查看日志）
  ## local.dir设置为数据盘，避免使用系统分区
  spark.local.dir=/tmp/spark_temp_data
  ## 优化设置
  spark.kryoserializer.buffer.max=512m
  spark.serializer=org.apache.spark.serializer.KryoSerializer
  spark.authenticate=false   （关闭数据块传输服务SASL加密认证）
  spark.io.encryption.enabled=false   （关闭I/O加密）
  spark.network.crypto.enabled=false  （关闭基于AES算法的RPC加密）
  spark.shuffle.service.enabled=true  （启用外部ShuffleService提高Shuffle稳定性）
  spark.shuffle.service.port=7337  （这个外部ShuffleService由YarnNodeManager提供，默认端口7337）
  spark.shuffle.useOldFetchProtocol=true  （兼容旧的Shuffle协议避免报错）
  spark.sql.cbo.enabled=true  (启用CBO基于代价的优化-代替RBO基于规则的优化-Optimizer)
  spark.sql.cbo.starSchemaDetection=true  （星型模型探测，判断列是否是表的主键）
  spark.sql.datetime.java8API.enabled=false
  spark.sql.sources.partitionOverwriteMode=dynamic 
  spark.sql.orc.mergeSchema=true  （ORC格式Schema加载时从所有数据文件收集）
  spark.sql.parquet.mergeSchema=false (根据情况设置，我们集群大多数都是parquet，从所有文件收集Schema会影响性能，所以从随机一个Parquet文件收集Schema)
  spark.sql.parquet.writeLegacyFormat=true  （兼容旧集群）
  spark.sql.autoBroadcastJoinThreshold=1048576  （当前仅支持运行了ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscan的Hive Metastore表，以及直接在数据文件上计算统计信息的基于文件的数据源表）
  spark.sql.adaptive.enabled=true   （Spark AQE[adaptive query execution]启用，AQE的优势：执行计划可动态调整、调整的依据是中间结果的精确统计信息）
  spark.sql.adaptive.forceApply=false
  spark.sql.adaptive.logLevel=info
  spark.sql.adaptive.advisoryPartitionSizeInBytes=256m  （倾斜数据分区拆分，小数据分区合并优化时，建议的分区大小，与spark.sql.adaptive.shuffle.targetPostShuffleInputSize含义相同）
  spark.sql.adaptive.coalescePartitions.enabled=true  （是否开启合并小数据分区默认开启，调优策略之一）
  spark.sql.adaptive.coalescePartitions.minPartitionSize=1m  （合并后最小的分区大小）
  spark.sql.adaptive.coalescePartitions.initialPartitionNum=1024  （合并前的初始分区数）
  spark.sql.adaptive.fetchShuffleBlocksInBatch=true  （是否批量拉取blocks,而不是一个个的去取，给同一个map任务一次性批量拉取blocks可以减少io 提高性能）
  spark.sql.adaptive.localShuffleReader.enabled=true （不需要Shuffle操作时，使用LocalShuffleReader，例如将SortMergeJoin转为BrocastJoin）
  spark.sql.adaptive.skewJoin.enabled=true   （Spark会通过拆分的方式自动处理Join过程中有数据倾斜的分区）
  spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes=128m
  spark.sql.adaptive.skewJoin.skewedPartitionFactor=5  （判断倾斜的条件：分区大小大于所有分区大小中位数的5倍，且大于spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes的值）
  ## 默认应用资源设置
  spark.driver.memory=2G
  spark.executor.cores=4
  spark.executor.memory=4G
  spark.executor.memoryOverhead=2G
  spark.memory.offHeap.enabled=true
  spark.memory.offHeap.size=2G
  ## 动态资源设置  具体逻辑见ExecutorAllocationManager这个类
  spark.dynamicAllocation.enabled=true
  spark.dynamicAllocation.executorIdleTimeout=60 （executor闲置时间，如果某executor空闲超过60s，则remove此executor）
  spark.dynamicAllocation.minExecutors=0
  spark.dynamicAllocation.schedulerBacklogTimeout=5s  （如果有pending task并且等待了5s，则申请增加executor）
  spark.dynamicAllocation.cachedExecutorIdleTimeout=600 （cache闲置时间，超过此时间，可释放cache所在的executor）
  ## 其他设置
  spark.driver.extraLibraryPath=/opt/cloudera/parcels/CDH/lib/hadoop/lib/native
  spark.executor.extraLibraryPath=/opt/cloudera/parcels/CDH/lib/hadoop/lib/native
  spark.yarn.am.extraLibraryPath=/opt/cloudera/parcels/CDH/lib/hadoop/lib/native
  spark.ui.enabled=true
  spark.ui.killEnabled=true
  spark.master=yarn
  spark.sql.hive.metastore.version=2.1.1
  spark.sql.hive.metastore.jars=/opt/cloudera/parcels/CDH/lib/hive/lib/*
# 修改spark-env.sh
vim spark-env.sh
  export JAVA_HOME=/usr/java/jdk1.8.0_181
  HADOOP_CONF_DIR=/etc/hadoop/conf
  export SPARK_DIST_CLASSPATH=$(/opt/cloudera/parcels/CDH/bin/hadoop classpath)
  export SPARK_LOCAL_DIRS=/tmp/spark_temp_data
# 软连接hive和hdfs、yarn配置：
ln -s /etc/hadoop/conf/core-site.xml core-site.xml
ln -s /etc/hbase/conf/hbase-site.xml hbase-site.xml
ln -s /etc/hadoop/conf/hdfs-site.xml hdfs-site.xml
ln -s /etc/hive/conf/hive-site.xml hive-site.xml
ln -s /etc/hadoop/conf/mapred-site.xml mapred-site.xml
ln -s /etc/hadoop/conf/yarn-site.xml yarn-site.xml
# 将CRLF转LF以保证运行正常
cd spark-3.2.2-bin-hadoop-3.0.0-cdh6.3.2
sudo yum -y install dos2unix
dos2unix bin/*
dos2unix sbin/*
dos2unix conf/*
dos2unix python/*
# 运行SparkHistoryServer (注意该进程会产生日志，为避免占用系统分区空间，尽量将$SPARK_HOME/logs软连接到数据盘)
sbin/start-history-server.sh
# 运行SparkSQL on Yarn
bin/spark-sql --master yarn </code></pre>
<p><strong>至此Spark3.2.2 On CDH6.3.2编译部署完毕</strong><br>注意Yarn外部ShuffleService一定确保开启<br><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Kyuubi/Kyuubi-06.png" alt="alt"></p>
<h3 id="Kyuubi-On-Spark3基础部署"><a href="#Kyuubi-On-Spark3基础部署" class="headerlink" title="Kyuubi On Spark3基础部署"></a>Kyuubi On Spark3基础部署</h3><p>更多配置参考：<a href="https://kyuubi.apache.org/docs/latest/deployment/settings.html" target="_blank" rel="noopener">Kyuubi-Deployment-Settings</a><br><strong>安装与配置Kyuubi</strong></p>
<pre><code class="shell">wget https://www.apache.org/dyn/closer.lua/incubator/kyuubi/kyuubi-1.5.1-incubating/apache-kyuubi-1.5.1-incubating-bin.tgz
tar -zxvf apache-kyuubi-1.5.1-incubating-bin.tgz -C /opt/modules/
# 设置环境变量 vim /etc/profile
# KYUUBI_HOME
export KYUUBI_HOME=/opt/modules/apache-kyuubi-1.5.1-incubating-bin
# 配置文件修改
cd apache-kyuubi-1.5.1-incubating-bin
cd conf
cp kyuubi-env.sh.template  kyuubi-env.sh;cp kyuubi-defaults.conf.template kyuubi-defaults.conf;cp log4j2.properties.template log4j2.properties
# 修改kyuubi-env.sh
vim kyuubi-env.sh
  export JAVA_HOME=/usr/java/jdk1.8.0_181
  export SPARK_HOME=/opt/modules/spark-3.2.2-bin-hadoop-3.0.0-cdh6.3.2
  export HADOOP_CONF_DIR=/etc/hive/conf
  export KYUUBI_JAVA_OPTS=&quot;-Xmx4g -XX:+UnlockDiagnosticVMOptions -XX:ParGCCardsPerStrideChunk=4096 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSConcurrentMTEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -XX:+CMSClassUnloadingEnabled -XX:+CMSParallelRemarkEnabled -XX:+UseCondCardMark -XX:MaxDirectMemorySize=1024m  -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=./logs -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -Xloggc:./logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=5M -XX:NewRatio=3 -XX:MetaspaceSize=512m&quot;
# 修改kyuubi-defaults.conf （由于我之前的Spark安装中已经配置了hive-site等配置，这里不需要指定hive相关配置了，正常这里是可以指定hive配置的，参考https://kyuubi.apache.org/docs/latest/deployment/hive_metastore.html）
vim kyuubi-defaults.conf
  spark.master=yarn
  kyuubi.ha.zookeeper.acl.enabled=true
  kyuubi.ha.zookeeper.quorum=cdh101:2181,cdh102:2181,cdh103:2181
  kyuubi.engine.share.level=USER
  kyuubi.session.engine.idle.timeout=PT1H
  spark.dynamicAllocation.enabled=true
  spark.dynamicAllocation.minExecutors=1
  spark.dynamicAllocation.maxExecutors=10
  spark.dynamicAllocation.executorIdleTimeout=120</code></pre>
<p><strong>启动与连接Kyuubi</strong></p>
<pre><code class="shell"># 启动Kyuubi Server
bin/kyuubi start
# 使用hive用户 连接Kyuubi
beeline -u jdbc:hive2://10.2.5.101:10009 -n hive
show databases  # 该命令直接触发Spark引擎初始化</code></pre>
<p><strong>至此Kyuubi基础配置完成</strong><br>Kyuubi申请到Spark引擎后，默认空闲30min后自动回收。</p>
<h3 id="Kyuubi生产环境的高级配置"><a href="#Kyuubi生产环境的高级配置" class="headerlink" title="Kyuubi生产环境的高级配置"></a>Kyuubi生产环境的高级配置</h3><p>在上面基础配置的基础上增加生产环境所需的高级配置，包括安全性配置，用户的配置，授权配置等。<br>Kerberos认证</p>
<pre><code class="config">kyuubi.kinit.keytab=/hadoop/bigdata/kerberos/keytab/hive.keytab
kyuubi.kinit.principal=hive/xxx@XXX.COM</code></pre>
<p>采用LDAP认证 使用LDAP认证登陆Kyuubi</p>
<pre><code class="config">kyuubi.authentication=LDAP
##kyuubi.authentication.ldap.base.dn=
kyuubi.authentication.ldap.domain=xxxx.com
kyuubi.authentication.ldap.url=ldap://xxx.xx.xx.xxx</code></pre>
<p>使用q00885用户登陆，执行sql查询，后台会以q00885申请一个SparkApplication。<br><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Kyuubi/Kyuubi-03.png" alt="alt"><br>查询时，数据访问、元数据访问都使用这个用户，要确保这个用户有HDFS上ACL权限(hdfs dfs -getfacl查看)。<br>还要确保Linux上有该用户，否则引擎无法申请成功。<br>如果没有HDFS上的ACL权限，可以通过setfacl设置ACL,或者通过hive的grant命令针对组批量授权。</p>
<h3 id="集成Kudu"><a href="#集成Kudu" class="headerlink" title="集成Kudu"></a>集成Kudu</h3><p><a href="https://kyuubi.apache.org/docs/latest/integrations/kudu.html#" target="_blank" rel="noopener">Kyuubi On Kudu</a></p>
<h3 id="Kyuubi的授权："><a href="#Kyuubi的授权：" class="headerlink" title="Kyuubi的授权："></a>Kyuubi的授权：</h3><p>Kyuuib当前支持三种授权：<br>1.基于存储层面的授权(以上我们使用到的授权方式)<br>2.基于SQL标准的授权类似HiveServer2(基于<a href="https://mvnrepository.com/artifact/org.apache.submarine/submarine-spark-security" target="_blank" rel="noopener">Submarine:Spark Security</a>外部插件)<br>3.基于Ranger(官网推荐，也是基于Submarine Spark，只是通过Spark-Ranger来实现更细粒度的访问授权)</p>
<h3 id="问题与异常处理"><a href="#问题与异常处理" class="headerlink" title="问题与异常处理"></a>问题与异常处理</h3><p><a href="https://kyuubi.apache.org/docs/latest/monitor/trouble_shooting.html" target="_blank" rel="noopener">Kyuubi Trouble Shooting</a></p>
<ol>
<li><p>执行spark sql后一直卡住，后台报错User: root is not allowed to impersonate anonymous</p>
<pre><code class="log">Error: org.apache.kyuubi.KyuubiSQLException: Timeout(180000 ms) to launched SPARK_SQL engine with /opt/modules/spark-3.2.2-bin-hadoop-3.0.0-cdh6.3.2/bin/spark-submit \
     --class org.apache.kyuubi.engine.spark.SparkSQLEngine \
     --conf spark.kyuubi.ha.zookeeper.quorum=cdh101:2181,cdh102:2181,cdh103:2181 \
     --conf spark.kyuubi.client.ip=10.2.5.101 \
     --conf spark.hive.query.redaction.rules=/etc/alternatives/hive-conf/redaction-rules.json \
     --conf spark.kyuubi.engine.submit.time=1651991699800 \
     --conf spark.app.name=kyuubi_USER_SPARK_SQL_anonymous_default_a0c93d16-2718-4791-8205-97fbc35e652a \
     --conf spark.kyuubi.ha.zookeeper.acl.enabled=true \
     --conf spark.kyuubi.ha.engine.ref.id=a0c93d16-2718-4791-8205-97fbc35e652a \
     --conf spark.kyuubi.ha.zookeeper.auth.type=NONE \
     --conf spark.master=yarn \
     --conf spark.yarn.tags=KYUUBI \
     --conf spark.kyuubi.ha.zookeeper.namespace=/kyuubi_1.5.1-incubating_USER_SPARK_SQL/anonymous/default \
     --conf spark.hive.exec.query.redactor.hooks=org.cloudera.hadoop.hive.ql.hooks.QueryRedactor \
     --proxy-user anonymous /opt/modules/apache-kyuubi-1.5.1-incubating-bin/externals/engines/spark/kyuubi-spark-sql-engine_2.12-1.5.1-incubating.jar. (state=,code=0)
......
22/05/08 14:37:17 INFO retry.RetryInvocationHandler: org.apache.hadoop.security.authorize.AuthorizationException: User: root is not allowed to impersonate anonymous, while invoking ApplicationClientProtocolPBClientImpl.getClusterMetrics over null after 5 failover attempts. Trying to failover after sleeping for 37639ms.</code></pre>
<p>解决：避免使用root用户启动Kyuubi Server。可使用hive、hdfs用户启动，或单独建立一个kyuubi用户启动KyuubiServer。</p>
</li>
<li><p>用户无目录以及NM节点没用户导致引擎无法运行</p>
<pre><code class="error">Caused by: org.apache.kyuubi.KyuubiSQLException: Timeout(180000 ms) to launched SPARK_SQL engine with /data3/bigdata/spark/spark-3.2.2-bin-hadoop-3.0.0-cdh6.3.2/bin/spark-submit \
     --class org.apache.kyuubi.engine.spark.SparkSQLEngine \
     --conf spark.kyuubi.authentication.ldap.url=ldap://xxx.xx.xxx.xx \
     --conf spark.kyuubi.ha.zookeeper.quorum=zk1:2181,zk2:2181,zk3:2181 \
     --conf spark.kyuubi.client.ip=xxx.xx.xxx.xx \
     --conf spark.kyuubi.kinit.principal=hive/hive02.c6.com@XXX.COM \
     --conf spark.kyuubi.engine.submit.time=1652671391562 \
     --conf spark.app.name=kyuubi_USER_SPARK_SQL_k00877_default_ff13fda9-1a01-4322-8d40-d3bc098d78e4 \
     --conf spark.kyuubi.ha.zookeeper.acl.enabled=true \
     --conf spark.kyuubi.ha.engine.ref.id=ff13fda9-1a01-4322-8d40-d3bc098d78e4 \
     --conf spark.master=yarn \
     --conf spark.yarn.tags=KYUUBI \
     --conf spark.kyuubi.ha.zookeeper.namespace=/kyuubi_1.5.1-incubating_USER_SPARK_SQL/k00877/default \
     --conf spark.kyuubi.kinit.keytab=/hadoop/bigdata/kerberos/keytab/hiveserver2_hive02_c6.keytab \
     --conf spark.kyuubi.authentication.ldap.domain=smyoa.com \
     --proxy-user k00877 /data3/bigdata/spark/apache-kyuubi-1.5.1-incubating-bin/externals/engines/spark/kyuubi-spark-sql-engine_2.12-1.5.1-incubating.jar. 
     at org.apache.kyuubi.KyuubiSQLException$.apply(KyuubiSQLException.scala:69) ~[kyuubi-common_2.12-1.5.1-incubating.jar:1.5.1-incubating]
     ......
Caused by: org.apache.kyuubi.KyuubiSQLException: org.apache.hadoop.security.AccessControlException: Permission denied: user=k00877, access=WRITE, inode=&quot;/user&quot;:hdfs:supergroup:drwxr-xr-x
     at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:400)
     at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:256)
     at org.apache.sentry.hdfs.SentryINodeAttributesProvider$SentryPermissionEnforcer.checkPermission(SentryINodeAttributesProvider.java:86)
     at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:194)</code></pre>
<p>尝试在HDFS上创建对应用户目录  (这里也可以修改Spark在文件系统中当前用户的主目录-提交应用的缓存目录：spark.yarn.stagingDir)</p>
<pre><code class="shell">hdfs dfs -mkdir /user/k00877/
hdfs dfs -chown k00877:k00877 /user/k00877/</code></pre>
<p>再次尝试创建引擎，报错如下  </p>
<pre><code class="error">Kyuubi Server error：
Caused by: org.apache.kyuubi.KyuubiSQLException: org.apache.spark.SparkException: Application application_1637826239096_34377 failed 2 times due to AM Container for appattempt_1637826239096_34377_000002 exited with  exitCode: -1000
See more: /hadoop/bigdata/spark/apache-kyuubi-1.5.1-incubating-bin/work/k00877/kyuubi-spark-sql-engine.log.4
     at org.apache.kyuubi.KyuubiSQLException$.apply(KyuubiSQLException.scala:69) ~[kyuubi-common_2.12-1.5.1-incubating.jar:1.5.1-incubating]
     at org.apache.kyuubi.engine.ProcBuilder.$anonfun$start$1(ProcBuilder.scala:165) ~[kyuubi-server_2.12-1.5.1-incubating.jar:1.5.1-incubating]
Engine log: /hadoop/bigdata/spark/apache-kyuubi-1.5.1-incubating-bin/work/k00877/kyuubi-spark-sql-engine.log.4
For more detailed output, check the application tracking page: http://xxxxx:8088/cluster/app/application_1637826239096_34377 Then click on links to logs of each attempt.
. Failing the application.
org.apache.spark.SparkException: Application application_1637826239096_34377 failed 2 times due to AM Container for appattempt_1637826239096_34377_000002 exited with  exitCode: -1000
Failing this attempt.Diagnostics: [2022-05-16 13:00:12.276]Application application_1637826239096_34377 initialization failed (exitCode=255) with output: main : command provided 0
main : run as user is k00877
main : requested yarn user is k00877
User k00877 not found
......</code></pre>
<p>在一个没有开启Kerberos安全的集群里，启动container进程可以使用DefaultContainerExecutor或LinuxContainerExecutor；但是启用了Kerberos安全的集群里，启动container进程只能使用LinuxContainerExecutor，在底层会使用setuid切换到业务用户以启动container进程，所以要求所有nodemanager节点必须有业务用户。<br>解决：首先保证用户主目录有权限的前提下，在各个NodeManager节点创建k00877用户，创建后可以看到引擎正常启动<br><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Kyuubi/Kyuubi-05.png" alt="alt"></p>
</li>
<li><p>使用LDAP登录的用户无HDFS上表数据的访问权限<br><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Kyuubi/Kyuubi-04.png" alt="alt"><br>分析：需要确保当前用户的权限或者ACL权限是READ_EXECUTE<br><img src="https://cdn.jsdelivr.net/gh/Shmilyqjj/BlogImages-0@master/cdn_sources/Blog_Images/Kyuubi/Kyuubi-07.png" alt="alt"><br>当前用户q00885没有该目录的任何读权限。解决方式：</p>
</li>
</ol>
<pre><code class="text"> 使用hive用户登录HiveServer2：beeline -u &quot;jdbc:hive2://172.18.204.199:10000/default&quot; -nhive -pxxxxx
 查看q00885所属角色
 SHOW ROLE GRANT GROUP group q00885;
 +--------+---------------+-------------+----------+--+
 |  role  | grant_option  | grant_time  | grantor  |
 +--------+---------------+-------------+----------+--+
 | admin  | false         | 0           | --       |
 | d_bd   | false         | 0           | --       |
 +--------+---------------+-------------+----------+--+
 授权权限给d_bd角色
 grant select on table t_sai_t_model_log to role d_bd;
 查看d_bd角色有哪些权限
 SHOW GRANT ROLE d_bd;
+-------------------------------------+----------------------------------------+------------+---------+-----------------+-----------------+------------+---------------+----------------+----------+--+
|              database               |                 table                  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |   grant_time   | grantor  |
+-------------------------------------+----------------------------------------+------------+---------+-----------------+-----------------+------------+---------------+----------------+----------+--+
| default                             | xxxxxxxxxx                |            |         | d_bd            | ROLE            | SELECT     | false         | 1629844007000  | --       |
| default                             | t_sai_t_model_log                      |            |         | d_bd            | ROLE            | SELECT     | false         | 1652777345000  | --       |
| default                             | xxxxxxxxxx       |            |         | d_bd            | ROLE            | SELECT     | false         | 1634268085000  | --       |
+-------------------------------------+----------------------------------------+------------+---------+-----------------+-----------------+------------+---------------+----------------+----------+--+
 先回收权限，测试另一种方法：设置acl
 revoke select on table t_sai_t_model_log from role d_bd;
 给表数据路径增加ACL权限
 hdfs dfs -setfacl -R -m group:q00885:r-x /user/hive/warehouse/t_sai_t_model_log
 设置ACL后再用getfacl查看ACL列表，设置没生效，是因为我们集群用了Sentry管理ACL，直接对目录设置ACL不会生效，所以还需使用hive的grant+revoke方式授权。</code></pre>
<p>再次使用q00885即可查询。</p>
<pre><code class="text">权限列表: 
ALL SERVER, TABLE, DB, URI, COLLECTION, CONFIG
INSERT  DB, TABLE
SELECT  DB, TABLE, COLUMN
授权与回收：
GRANT ROLE &lt;role name&gt; [, &lt;role name&gt;] TO GROUP &lt;group name&gt; [,GROUP &lt;group name&gt;]
GRANT &lt;privilege&gt; [, &lt;privilege&gt; ] ON &lt;object type&gt; &lt;object name&gt; TO ROLE &lt;role name&gt; [,ROLE &lt;role name&gt;]
GRANT SELECT &lt;column name&gt; ON TABLE &lt;table name&gt; TO ROLE &lt;role name&gt;;
REVOKE ROLE &lt;role name&gt; [, &lt;role name&gt;] FROM GROUP &lt;group name&gt; [,GROUP &lt;group name&gt;]
REVOKE &lt;privilege&gt; [, &lt;privilege&gt; ] ON &lt;object type&gt; &lt;object name&gt; FROM ROLE &lt;role name&gt; [,ROLE &lt;role name&gt;]
REVOKE SELECT &lt;column name&gt; ON TABLE &lt;table name&gt; FROM ROLE &lt;role name&gt;;</code></pre>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://kyuubi.apache.org/docs/latest/index.html" target="_blank" rel="noopener">Apache Kyuubi Documents</a><br><a href="https://kyuubi.apache.org/docs/latest/deployment/settings.html" target="_blank" rel="noopener">Apache Kyuubi Deployment Settings</a><br><a href="https://spark.apache.org/docs/latest/configuration.html" target="_blank" rel="noopener">Apache Spark Configuration</a><br><a href="https://spark.apache.org/docs/latest/monitoring.html#spark-history-server-configuration-options" target="_blank" rel="noopener">spark-history-server-configuration-options</a><br><a href="https://blog.csdn.net/u013411339/article/details/107075125/" target="_blank" rel="noopener">SparkSQL的自适应执行</a><br><a href="https://spark.apache.org/docs/latest/sql-migration-guide.html#upgrading-from-spark-sql-24-to-30" target="_blank" rel="noopener">Migration Guide: SQL, Datasets and DataFrame</a></p>

</div>


    <div class="post-guide">
        <div class="item left">
            
              <a href="/welcome/">
                  <i class="fa fa-angle-left" aria-hidden="true"></i>
                  
              </a>
            
        </div>
        <div class="item right">
            
              <a href="/84534d72/">
                SeaTunnel开源数据同步平台
                <i class="fa fa-angle-right" aria-hidden="true"></i>
              </a>
            
        </div>
    </div>



	<div id="vcomments"></div>


<script>
	
		// 评论
		new Valine({
			av: AV,
			el: '#vcomments',
			notify: false,
			verify: false,
			path: window.location.pathname,
			appId: 'zUyVEaHo59RUUwiPTChPEeBj-gzGzoHsz',
			appKey: 'xIEyTcrkTuJLz6ewPbpTj8mz',
			placeholder: '欢迎评论...',
			avatar: 'retro',
			recordIP: false
		})
	
	
</script>
	</div>
	<div id="footer">
	<p>
	©2020-<span id="footerYear"></span> 
	<a href="/">佳境Shmily</a> 
	
	
		|
		<span id="busuanzi_container_site_pv">
			pv
			<span id="busuanzi_value_site_pv"></span>
		</span>
		|
		<span id="busuanzi_container_site_uv"> 
			uv
			<span id="busuanzi_value_site_uv"></span>
		</span>
	
	<br>
	Theme <a href="//github.com/wujun234/hexo-theme-tree" target="_blank">Tree</a>
	by <a href="//github.com/wujun234" target="_blank">WuJun</a>
	Powered by <a href="//hexo.io" target="_blank">Hexo</a>
	</p>
</div>
<script type="text/javascript"> 
	document.getElementById('footerYear').innerHTML = new Date().getFullYear() + '';
</script>
	<button id="totop-toggle" class="toggle"><i class="fa fa-angle-double-up" aria-hidden="true"></i></button>
</body>
</html>